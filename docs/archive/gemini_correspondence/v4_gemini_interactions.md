**Initiating Comprehension**

I'm starting by really grasping the core objective: generating project interactions across multiple levels (0-3) using the `docs/big_picture` files. My initial focus is on extracting meaningful connections and relationships from those files. I'm building a solid foundation of understanding before posing any clarifying questions.


**Defining Module Relationships**

I've moved beyond basic project overviews and am now focused on extracting detailed interactions. I'm carefully examining each project description, looking for phrases indicating connections like "integrates with," "depends on," or "provides services to." I'm now starting to identify specific modules and their relationships at the different interaction levels.


**Formulating Connections**

I'm now focused on fleshing out interactions, starting with level 0 (direct calls). I'm mapping potential connections between modules based on keywords like "integrates" and "uses," along with the "Key Capabilities" sections. I'm noting where one project might directly invoke a function or access resources of another. The goal is to identify explicit dependencies.


**Developing Interaction Logic**

I'm now mapping interactions, starting with direct calls (level 0). Considering each module's core functions (e.g., `search_papers`, `get_transcript`), I am now building a list of likely level 0 interactions. Also, I'm noting where they produce outputs usable by others. I'm also starting to think through simple sequential chains (level 1).


**Developing More Connections**

I'm now working on extending the chains beyond level 1, focusing on levels 2 and 3. This requires identifying potential branching scenarios and orchestrations. Considering how modules can work together in parallel and how `claude-module-communicator` can act as an orchestrator. I'm starting to build interaction diagrams, focusing on the flow of information between projects, including those implied by their "Ecosystem Role".


**Synthesizing Interaction Ideas**

I'm now combining the extracted project details with the understanding of interaction levels to brainstorm specific connections. I am meticulously documenting the flow of data between modules, identifying inputs and outputs for each. This is helping me create a map of potential interactions that aligns with the specified requirements.


**Elaborating Interactions**

I'm now structuring my output to follow the specified format: clear titles, involved modules, flow descriptions, justification for the level, and potential pseudo-code, along with Mermaid diagrams. This includes the generation of novel interaction ideas, and I am building up the interactions. I'm checking to ensure all outputs are easily understandable, and meet the requirements for different interaction levels, and have added some examples.


**Generating System Interactions**

I'm now focusing on applying the understanding gained from the project overviews, focusing on the "Key Capabilities" sections and "Ecosystem Role". I'm using these to generate various levels (0-3) interaction examples, and I'm thinking about how they relate to each other. I'm now working on the more complex interactions.


Okay, this is a rich dataset! The provided `docs/big_picture` files, especially the `MODULE_INTERACTION_LEVELS.md` and `INTERACTION_EXAMPLES_VISUAL.md`, give excellent guidance on what constitutes each interaction level. I'll leverage the "Core Capabilities" and "Integration Points" of each module to create new, distinct interactions.

I will focus on interactions *not explicitly listed* in the example files, or build upon their concepts with different module combinations.

Let's generate some new interactions across all four levels.

---

## New Project Interactions

### ðŸŽ¯ Level 0: Direct Module Calls (Basic)

These are simple, independent calls to a single module performing its core function.

---

#### 0.1 Document Integrity Check
*   **Involved Modules**: `marker.py` (specifically core `marker` functions related to validation)
*   **Flow Description**: A user or system requests `marker` to perform a basic integrity check on a PDF document (e.g., checking for corruption, basic readability, or count of pages).
*   **Why this level**: Single, self-contained operation within `marker` without external dependencies for this specific task.
*   **Pseudo-Code**:
    ```python
    # Using an assumed Marker API function for file integrity
    from marker import MarkerSDK # Assuming a SDK or direct module access

    marker_sdk = MarkerSDK()
    is_valid = marker_sdk.check_document_integrity("path/to/my_document.pdf")
    print(f"Document integrity valid: {is_valid}")
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        User[User/System] -->|check_document_integrity("doc.pdf")| Marker[marker]
        Marker -->|boolean result| User
    ```

---

#### 0.2 LLM Model Selection & Configuration
*   **Involved Modules**: `claude_max_proxy.py`
*   **Flow Description**: A user or system directly queries `claude_max_proxy` to list available LLMs or retrieve configuration details for a specific model without performing an actual LLM call.
*   **Why this level**: Single module operation, retrieving internal state/configuration.
*   **Pseudo-Code**:
    ```python
    from claude_max_proxy import LLMProxy

    llm_proxy = LLMProxy()
    available_models = llm_proxy.list_available_models()
    model_config = llm_proxy.get_model_config("claude-3-opus-20240229")
    print(f"Available models: {available_models}")
    print(f"Claude Opus config: {model_config}")
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        User[User/System] -->|list_available_models()| ClaudeMaxProxy[claude_max_proxy]
        ClaudeMaxProxy -->|model list/config| User
    ```

---

#### 0.3 Registry Lookup
*   **Involved Modules**: `shared_claude_docs.py` (specifically `module_registry.py` functionality if exposed)
*   **Flow Description**: A system directly queries the shared Claude documentation/registry to get metadata about a specific registered project.
*   **Why this level**: A direct lookup within one centralized documentation/registry module.
*   **Pseudo-Code**:
    ```python
    from shared_claude_docs import ModuleRegistry # Assumed access to registry within shared_claude_docs

    registry = ModuleRegistry()
    project_metadata = registry.get_project_info("sparta")
    print(f"Sparta metadata: {project_metadata}")
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        System[System] -->|get_project_info("sparta")| SharedDocs[shared_claude_docs]
        SharedDocs -->|project metadata| System
    ```

---

### ðŸ”— Level 1: Sequential Chain (Pipeline)

Output from one module feeds directly into the input of another in a linear fashion.

---

#### 1.1 Academic Content Summarization & Storage
*   **Involved Modules**: `arxiv-mcp-server`, `marker`, `sparta`, `arangodb`
*   **Flow Description**: Fetch a research paper, extract its abstract/main text, apply a summarization model, and then store the summarized content and metadata in the knowledge graph.
*   **Why this level**: Each step is strictly dependent on the previous one's output to proceed.
*   **Pseudo-Code / Data Flow**:
    ```
    User Request (paper_id)
    â†“
    arxiv-mcp-server.fetch_paper(paper_id)
    â†“ (PDF Content)
    marker.extract_text(pdf_content, area="abstract")
    â†“ (Extracted Abstract Text)
    sparta.summarize_text(abstract_text)
    â†“ (Summarized Text)
    arangodb.store_document_summary(paper_id, summarized_text)
    â†“ (Confirmation/Graph ID)
    Result to User
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        User[User] -->|paper ID| Arxiv[arxiv-mcp-server]
        Arxiv -->|PDF| Marker[marker]
        Marker -->|Abstract Text| Sparta[sparta]
        Sparta -->|Summarized Text| ArangoDB[arangodb]
        ArangoDB -->|Graph ID| User
    ```

---

#### 1.2 Training Data Generation Pipeline
*   **Involved Modules**: `youtube_transcripts`, `marker`, `fine_tuning`, `arangodb`
*   **Flow Description**: Retrieve a video transcript, use `marker` to extract Q&A pairs (or specific factual statements) from it, potentially prepare this data for fine-tuning via `fine_tuning` (e.g., reformatting), and then finally store the structured dataset in `arangodb` for model training.
*   **Why this level**: The data sequentially transforms and moves, with each module building on the previous one's output.
*   **Pseudo-Code / Data Flow**:
    ```
    Video ID
    â†“
    youtube_transcripts.get_transcript(video_id)
    â†“ (Raw Transcript)
    marker.extract_qa_pairs_from_text(transcript) # Assumed feature of marker
    â†“ (Structured Q&A Pairs)
    fine_tuning.preprocess_for_training(qa_pairs) # Assumed utility for specific training format
    â†“ (Formatted Training Data)
    arangodb.store_training_data(dataset_name, formatted_data)
    â†“ (Confirmation)
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        User[User] -->|video_id| YouTube[youtube_transcripts]
        YouTube -->|Transcript| Marker[marker]
        Marker -->|Q&A Pairs| Unsloth[fine_tuning]
        Unsloth -->|Formatted Data| ArangoDB[arangodb]
        ArangoDB -->|Confirmation| User
    ```

---

#### 1.3 Compliance-Driven Code Cleanup
*   **Involved Modules**: `marker`, `claude_compliance_checker`, `enhanced_cleanup`
*   **Flow Description**: A code repository is treated as a "document" by `marker` to extract Python files. `claude_compliance_checker` then analyzes these files for compliance issues. Finally, `enhanced_cleanup` (which can include automated fixes) is applied based on the compliance report.
*   **Why this level**: A clear, linear application of analysis and remediation steps.
*   **Pseudo-Code / Data Flow**:
    ```
    Codebase Path
    â†“
    marker.get_python_files(codebase_path) # Assumed marker feature
    â†“ (List of Python file paths)
    claude_compliance_checker.scan_files(python_files)
    â†“ (Compliance Report)
    enhanced_cleanup.apply_fixes_from_report(compliance_report) # Assumes enhanced_cleanup can read such a report
    â†“ (Cleanup Confirmation)
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph LR
        User[User] -->|Codebase Path| Marker[marker]
        Marker -->|Python Files| ComplianceChecker[claude_compliance_checker]
        ComplianceChecker -->|Compliance Report| Cleanup[enhanced_cleanup]
        Cleanup -->|Cleanup Confirmation| User
    ```

---

### ðŸ”€ Level 2: Parallel & Branching Workflows

Multiple modules execute in parallel, or workflow paths diverge based on conditional logic.

---

#### 2.1 Multi-faceted Code Analysis & Documentation
*   **Involved Modules**: `marker`, `claude_compliance_checker`, `claude-test-reporter`, `shared_claude_docs`, `arangodb`
*   **Flow Description**: A code repository is extracted by `marker`. In parallel:
    1.  `claude_compliance_checker` scans extracted code for compliance.
    2.  `claude-test-reporter` runs tests against the code and generates a report.
    Based on the compliance scan result, `shared_claude_docs` might update documentation. The combined results (compliance, test, and documentation status) are then stored in `arangodb`.
*   **Why this level**: Parallel execution of analysis, and a conditional branch (documentation update only if relevant compliance changes).
*   **Pseudo-Code / Data Flow**:
    ```
    Codebase Path
    â†“
    marker.extract_code_context(codebase_path) # Extracts files, simple structure, etc.
    â†“ (Code Context)
    â”€â”€â”€(Parallel Execution)â”€â”€â”€
    â”œâ”€â†’ claude_compliance_checker.scan(code_context)
    â”‚   â†“ (Compliance Findings)
    â”‚   IF compliance_findings.is_critical:
    â”‚       shared_claude_docs.flag_critical_compliance_issue(compliance_findings)
    â”‚   ELSE:
    â”‚       # Do nothing or minor doc update
    â”‚
    â””â”€â†’ claude-test-reporter.run_tests(code_context)
        â†“ (Test Report)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(Aggregation)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    All results (Compliance Findings +, Test Report)
    â†“
    arangodb.store_code_analysis(codebase_id, compliance_findings, test_report, doc_status)
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph TD
        User[User] -->|Codebase Path| Marker[marker]
        Marker --> CodeContext[Code Context]

        subgraph Parallel Analysis
            CodeContext --> Compliance[claude_compliance_checker]
            CodeContext --> TestReporter[claude-test-reporter]
        end

        Compliance --> ComplianceFindings[Compliance Findings]
        TestReporter --> TestReport[Test Report]

        ComplianceFindings --> Condition{Critical Compliance?}
        Condition -- Yes --> SharedDocs[shared_claude_docs]
        Condition -- No --> Null[No Action]

        subgraph Aggregation
            ComplianceFindings & TestReport & SharedDocs --> ArangoDB[arangodb]
        end

        ArangoDB --> Result[Analysis Stored]
    ```

---

#### 2.2 LLM Response Validation & Quality Steering
*   **Involved Modules**: `claude_max_proxy`, `marker-ground-truth`, `sparta`, `claude_compliance_checker`
*   **Flow Description**: A prompt is sent to `claude_max_proxy` to generate a response. In parallel:
    1.  `marker-ground-truth` validates the structure/format of the response (e.g., is it valid JSON, Markdown).
    2.  `claude_compliance_checker` analyzes the *content* of the response for safety, bias, or other compliance issues.
    `sparta` receives both the structural validation and compliance analysis. If the response passes both, `sparta` might mark it as high quality. If failures are detected, `sparta` could then trigger a re-prompt with specific instructions for `claude_max_proxy`.
*   **Why this level**: Parallel validation of different aspects of the LLM response, with conditional logic to trigger re-prompts or classify quality.
*   **Pseudo-Code / Data Flow**:
    ```
    User Prompt
    â†“
    claude_max_proxy.generate_response(user_prompt)
    â†“ (LLM Response)
    â”€â”€â”€(Parallel Validation)â”€â”€â”€
    â”œâ”€â†’ marker-ground-truth.validate_format(llm_response)
    â”‚   â†“ (Format Validation Result: True/False)
    â”‚
    â””â”€â†’ claude_compliance_checker.analyze_content(llm_response)
        â†“ (Content Compliance Result: Score, Issues)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(Aggregation & Conditional Action)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Format Validation Result + Content Compliance Result
    â†“
    sparta.assess_response_quality(format_ok, compliance_issues)
    â†“
    IF sparta.assessment_is_bad:
        claude_max_proxy.retry_generate(user_prompt, guidance=sparta.feedback)
        # Loop back or inform user
    ELSE:
        print("LLM response is high quality and compliant.")
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph TD
        User[User Prompt] --> ClaudeProxy[claude_max_proxy]
        ClaudeProxy --> LLMResponse[LLM Response]

        subgraph Parallel Validation
            LLMResponse --> MarkerGT[marker-ground-truth (Format)]
            LLMResponse --> ComplianceChecker[claude_compliance_checker (Content)]
        end

        MarkerGT --> FormatResult[Format Result]
        ComplianceChecker --> ContentResult[Content Result]

        subgraph Assessment & Action
            FormatResult & ContentResult --> Sparta[sparta (Quality Assess)]
            Sparta --> Condition{Response Good?}
        end

        Condition -- Yes --> Final[High Quality Response]
        Condition -- No --> ClaudeProxyRetry[claude_max_proxy (Retry with guidance)]
    ```

---

### ðŸŽ­ Level 3: Orchestrated Multi-Module Collaboration

Complex, adaptive systems with feedback loops, dynamic routing, state management, and often learning or optimization. `claude-module-communicator` would likely be the orchestrator.

---

#### 3.1 Adaptive Knowledge Synthesis & Model Improvement System
*   **Involved Modules**: `claude-module-communicator` (Orchestrator), `arxiv-mcp-server`, `youtube_transcripts`, `marker`, `sparta`, `marker-ground-truth`, `arangodb`, `fine_tuning`, `claude_max_proxy`, `claude-test-reporter`, `shared_claude_docs`
*   **Flow Description**: This system continuously scours academic and video content for new information on specified topics. It uses `marker` to extract key data. `sparta` analyzes this data, identifies knowledge gaps, and uses `claude_max_proxy` to generate research questions. New data is fed into `arangodb` to expand the knowledge graph. Critically, as new knowledge is integrated:
    *   `marker-ground-truth` is used to periodically validate the extraction accuracy of `marker`, and new validation datasets are gathered.
    *   `fine_tuning` fine-tunes specialized LLMs (using `claude_max_proxy` as a base via proxy) based on the refined knowledge graph and `marker-ground-truth` feedback, aiming to improve future question generation and summarization accuracy.
    *   `claude-test-reporter` continuously benchmarks the fine-tuned LLMs and `sparta`'s analytical modules.
    *   `shared_claude_docs` is dynamically updated with insights, summarization models, or extracted facts.
    The `claude-module-communicator` orchestrates this entire loop, adapting search queries, prioritizing data processing, and triggering model re-training based on performance metrics and knowledge graph growth.
*   **Why this level**: Continuous, cyclical process with multiple feedback loops (extraction-validation -> model improvement -> better extraction, knowledge growth -> new questions). Dynamic adaptation based on system performance and data availability. `claude-module-communicator` is essential for managing the state and dynamic routing.
*   **Orchestration Logic**:
    ```python
    class KnowledgeSynthesizer:
        def __init__(self, communicator: ClaudeModuleCommunicator):
            self.communicator = communicator
            self.knowledge_graph_state = arangodb.get_current_state()

        async def run_cycle(self, topic):
            # Phase 1: Discovery & Initial Extraction
            new_arxiv_data = await self.communicator.call(arxiv_mcp_server, "search_new_papers", topic)
            new_youtube_data = await self.communicator.call(youtube_transcripts, "search_new_videos", topic)

            extracted_arxiv = await self.communicator.call(marker, "batch_extract", new_arxiv_data)
            extracted_youtube = await self.communicator.call(marker, "batch_extract", new_youtube_data)

            # Phase 2: Knowledge Integration & Gap Analysis
            new_facts = await self.communicator.call(sparta, "semantic_fusion", extracted_arxiv, extracted_youtube)
            await self.communicator.call(arangodb, "update_knowledge_graph", new_facts)
            knowledge_gaps = await self.communicator.call(sparta, "identify_knowledge_gaps", self.knowledge_graph_state)

            # Phase 3: Adaptive Question Generation (drives further discovery)
            if knowledge_gaps:
                new_questions = await self.communicator.call(claude_max_proxy, "generate_research_questions", knowledge_gaps)
                # Feed new questions back into Discovery Phase for next cycle
                self.communicator.publish_event("new_research_questions", new_questions)

            # Phase 4: Model Improvement Loop (triggered by performance or data growth)
            if self.communicator.get_metric("marker_accuracy") < THRESHOLD or self.knowledge_graph_state.has_grown_significantly():
                validation_data = await self.communicator.call(marker_ground_truth, "get_unvalidated_extractions")
                if validation_data:
                    # Validate a subset
                    validated_data = await self.communicator.call(marker_ground_truth, "run_validation_pass", validation_data)
                    # Fine-tune marker's extraction model
                    await self.communicator.call(fine_tuning, "fine_tune_extraction_model", validated_data)
                    # Update marker's internal model or deploy new version via claude_max_proxy
                    self.communicator.publish_event("extraction_model_updated")

            # Phase 5: Continuous Evaluation & Documentation
            llm_benchmarks = await self.communicator.call(claude_test_reporter, "benchmark_all_llms")
            # Update documentation based on new knowledge/models
            await self.communicator.call(shared_claude_docs, "auto_update_knowledge_summary", self.knowledge_graph_state, llm_benchmarks)

            self.knowledge_graph_state = arangodb.get_current_state() # Update state for next cycle
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph TD
        A[Start Cycle: Topic] --> B{Discovery Phase}
        B --> C[arxiv-mcp-server: Search Papers]
        B --> D[youtube_transcripts: Search Videos]

        C --> E[marker: Extract Arxiv]
        D --> F[marker: Extract YouTube]

        E & F --> G[sparta: Semantic Fusion & Gap Analysis]
        G --> H[arangodb: Update Knowledge Graph]
        H -- new facts --> I[claude_max_proxy: Generate Research Questions]
        I -- new questions --> B

        H -- performance/data --> J{Model Improvement Loop}
        J --> K[marker-ground-truth: Get & Validate Extractions]
        K --> L[fine_tuning: Fine-tune Extraction Model]
        L --> M[claude_max_proxy: Deploy New Model]
        M --> E & F

        J --> N[claude-test-reporter: Benchmark LLMs]
        N & H --> O[shared_claude_docs: Auto-update Summary]
        O --> A
    ```

---

#### 3.2 Dynamic Test Environment & Automated Reporting System
*   **Involved Modules**: `claude-module-communicator` (Orchestrator), `claude_max_proxy`, `mcp-screenshot`, `claude-test-reporter`, `shared_claude_docs`, `arangodb`, `sparta`, `ui_self_improvement` (implied by name in shared_claude_docs)
*   **Flow Description**: This system provides a dynamic, LLM-driven test environment.
    1.  A user or service requests a test scenario description via `claude_max_proxy` (e.g., generate a test case for "login page").
    2.  `claude_max_proxy` generates step-by-step instructions (pseudo-code) and expected visual outcomes.
    3.  `mcp-screenshot` executes these instructions, capturing screenshots and interaction data.
    4.  `sparta` analyzes the captured screenshots against expected outcomes and identifies discrepancies or UI issues.
    5.  `claude-test-reporter` collects the test results, including visual diffs and performance metrics.
    6.  **Feedback Loop 1**: If anomalies are found, `claude_max_proxy` consults `sparta`'s analysis to either refine the test instructions **or** generate bug reports for `ui_self_improvement` (presumed to fix UI issues based on visual insights).
    7.  All results are logged in `arangodb` for historical trend analysis.
    8.  **Feedback Loop 2**: `shared_claude_docs` automatically updates, reflecting the current UI performance, common bugs, and generated test cases based on insights from `arangodb` and reports from `claude-test-reporter`.
    `claude-module-communicator` manages the adaptive execution, retry logic, and conditional branching, acting as the central intelligence.

*   **Why this level**: Highly adaptive, reactive, and self-improving. It involves continuous feedback loops between observation (`mcp-screenshot`, `sparta`), analysis (`sparta`, `claude-test-reporter`), and action (`claude_max_proxy` for test refinement, `ui_self_improvement` for fixes, `shared_claude_docs` for updates).
*   **Orchestration Logic**:
    ```python
    class TestAutomationOrchestrator:
        def __init__(self, communicator: ClaudeModuleCommunicator):
            self.communicator = communicator

        async def run_test_scenario(self, user_request):
            # Phase 1: Test Case Generation
            test_instructions = await self.communicator.call(claude_max_proxy, "generate_test_scenario", user_request)

            # Phase 2: Execution & Observation
            execution_results = await self.communicator.call(mcp_screenshot, "execute_and_capture", test_instructions)
            screenshots = execution_results.screenshots
            interaction_data = execution_results.interactions

            # Phase 3: Primary Analysis & Reporting (Parallel)
            visual_analysis_task = self.communicator.call(sparta, "analyze_visual_diff", screenshots, test_instructions.expected_visuals)
            test_report_task = self.communicator.call(claude_test_reporter, "generate_report", interaction_data, visual_analysis_task) # Awaits visual_analysis_task

            visual_analysis_results, test_report = await self.communicator.gather(visual_analysis_task, test_report_task)

            # Phase 4: Adaptive Feedback & Remediation
            if visual_analysis_results.anomalies_detected or test_report.has_failures:
                anomaly_details = {"visual": visual_analysis_results, "functional": test_report.failures}
                # Decide: Refine test OR report bug
                decision = await self.communicator.call(claude_max_proxy, "decide_remediation_strategy", anomaly_details, test_instructions)

                if decision.strategy == "refine_test":
                    new_instructions = await self.communicator.call(claude_max_proxy, "refine_test_instructions", test_instructions, anomaly_details)
                    self.communicator.publish_event("re_run_test_scenario", new_instructions) # Trigger re-run cycle
                elif decision.strategy == "report_bug":
                    bug_report = await self.communicator.call(claude_max_proxy, "generate_bug_report", anomaly_details)
                    await self.communicator.call(ui_self_improvement, "ingest_bug_report", bug_report) # Trigger UI fix
                    self.communicator.publish_event("bug_reported")

            # Phase 5: Persistence & Documentation (Continuous)
            persisted_id = await self.communicator.call(arangodb, "store_test_results", test_report, visual_analysis_results)
            await self.communicator.call(shared_claude_docs, "update_ui_performance_docs", persisted_id)

            return test_report
    ```
*   **Mermaid Diagram**:
    ```mermaid
    graph TD
        A[User Request/Service] --> B[claude_max_proxy: Generate Test Instructions]
        B --> C[mcp-screenshot: Execute & Capture]

        subgraph Parallel Analysis
            C --> D[sparta: Analyze Visual Diff]
            C --> E[claude-test-reporter: Generate Report]
        end

        D & E --> F{Feedback & Remediation}
        F -- Anomalies/Failures --> G[claude_max_proxy: Decide Remediation]
        G -- Refine Test --> B
        G -- Report Bug --> H[ui_self_improvement: Ingest Bug]

        F --> I[arangodb: Store Results]
        I --> J[shared_claude_docs: Update Docs]
        J --> K[End Cycle]
    ```