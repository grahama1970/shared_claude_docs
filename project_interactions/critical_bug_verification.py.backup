#!/usr/bin/env python3
"""
Module: critical_bug_verification.py
Description: Skeptically verify bug hunt results to ensure they represent real issues

External Dependencies:
- loguru: https://loguru.readthedocs.io/

Sample Input:
>>> verifier = CriticalBugVerifier()
>>> results = verifier.verify_bug_report("bug_hunt_report_20250608_090503.json")

Expected Output:
>>> print(results)
{
    "total_bugs_claimed": 10,
    "verified_real_bugs": 3,
    "false_positives": 7,
    "verification_confidence": 0.85
}
"""

import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
    level="INFO"
)


class CriticalBugVerifier:
    """Skeptically verify bug hunt results"""
    
    def __init__(self):
        self.verification_results = []
        self.false_positives = []
        self.confirmed_bugs = []
        
    def verify_bug_report(self, report_file: str) -> Dict[str, Any]:
        """Critically analyze a bug report"""
        logger.info(f"🔍 Starting critical verification of {report_file}")
        
        # Load report
        with open(report_file) as f:
            report = json.load(f)
            
        # Extract bugs from test history
        all_bugs = self._extract_all_bugs(report)
        
        logger.info(f"📊 Report claims {len(all_bugs)} bugs found")
        
        # Verify each bug
        for bug in all_bugs:
            verification = self._verify_single_bug(bug)
            self.verification_results.append(verification)
            
            if verification['is_real']:
                self.confirmed_bugs.append(bug)
            else:
                self.false_positives.append(bug)
                
        # Generate skeptical analysis
        analysis = self._generate_skeptical_analysis(report)
        
        return {
            'total_bugs_claimed': len(all_bugs),
            'verified_real_bugs': len(self.confirmed_bugs),
            'false_positives': len(self.false_positives),
            'verification_confidence': self._calculate_confidence(),
            'analysis': analysis,
            'recommendations': self._generate_recommendations()
        }
        
    def _extract_all_bugs(self, report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract all bugs from the report"""
        bugs = []
        
        # Get bugs from detailed_bugs if available
        if 'detailed_bugs' in report:
            bugs.extend(report['detailed_bugs'])
            logger.info(f"Found {len(bugs)} detailed bugs in report")
            
        # Get bugs from the bug database if available
        elif 'bug_database' in report:
            bugs.extend(report['bug_database'])
            
        # Reconstruct bugs from test history if no detailed bugs
        elif bugs == []:
            for test in report.get('test_history', []):
                if test.get('bugs_found', 0) > 0:
                    # For now, create placeholder bugs based on the scenario
                    for i in range(test['bugs_found']):
                        bugs.append({
                            'scenario': test['scenario'],
                            'timestamp': test['timestamp'],
                            'duration': test['duration'],
                            'bug_index': i,
                            'reconstructed': True
                        })
                    
        return bugs
        
    def _verify_single_bug(self, bug: Dict[str, Any]) -> Dict[str, Any]:
        """Verify if a single bug is real"""
        verification = {
            'bug': bug,
            'is_real': False,
            'reasons': [],
            'confidence': 0.0
        }
        
        # Check for suspicious patterns
        suspicious_patterns = []
        
        # 1. Check if bug has actual details
        if bug.get('reconstructed', False):
            suspicious_patterns.append("Bug lacks detailed description")
            
        # 2. Check test duration
        duration = bug.get('duration', 0)
        if duration < 0.01:
            suspicious_patterns.append(f"Test completed too fast: {duration}s")
            
        # 3. Check scenario type
        scenario = bug.get('scenario', '')
        if 'Memvid' in scenario and duration < 0.01:
            suspicious_patterns.append("Memvid test without cv2 dependency ran successfully")
            
        # 4. Check for missing authentication tests
        if 'Security' in scenario:
            if 'auth' in str(bug).lower() or 'authentication' in str(bug).lower():
                verification['is_real'] = True
                verification['reasons'].append("Security bug with auth implications")
            else:
                suspicious_patterns.append("Security test found bugs but no auth details")
                
        # 5. Check for ArangoDB connection issues
        if 'arangodb' in str(bug).lower():
            # These are likely real given the connection warnings in logs
            verification['is_real'] = True
            verification['reasons'].append("ArangoDB connection issues confirmed by logs")
            
        # Calculate confidence
        if suspicious_patterns:
            verification['confidence'] = max(0.0, 1.0 - (len(suspicious_patterns) * 0.25))
            verification['reasons'].extend(suspicious_patterns)
        else:
            verification['confidence'] = 0.8
            
        # Override for known real issues
        if verification['is_real']:
            verification['confidence'] = 0.9
            
        return verification
        
    def _generate_skeptical_analysis(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Generate skeptical analysis of the report"""
        analysis = {
            'credibility_issues': [],
            'positive_findings': [],
            'test_quality_score': 0.0
        }
        
        # Check test durations
        test_durations = [t['duration'] for t in report.get('test_history', [])]
        fast_tests = [d for d in test_durations if d < 0.01]
        
        if len(fast_tests) > len(test_durations) * 0.5:
            analysis['credibility_issues'].append(
                f"{len(fast_tests)}/{len(test_durations)} tests completed suspiciously fast"
            )
            
        # Check success rate
        success_rate = report.get('test_history', [])
        successful = [t for t in success_rate if t.get('success', False)]
        
        if len(successful) < len(success_rate) * 0.7:
            analysis['positive_findings'].append(
                f"Low success rate ({len(successful)}/{len(success_rate)}) indicates real issues"
            )
            
        # Check for missing modules
        if 'arxiv_mcp_server' in str(report) and 'import error' in str(report).lower():
            analysis['positive_findings'].append(
                "Import errors for arxiv_mcp_server are real syntax issues"
            )
            
        # Calculate test quality score
        quality_factors = {
            'has_real_durations': len(fast_tests) < len(test_durations) * 0.3,
            'has_failure_cases': len(successful) < len(success_rate),
            'has_error_details': 'error' in str(report).lower(),
            'has_module_variety': len(set(t['scenario'] for t in report.get('test_history', []))) > 3
        }
        
        analysis['test_quality_score'] = sum(quality_factors.values()) / len(quality_factors)
        
        return analysis
        
    def _calculate_confidence(self) -> float:
        """Calculate overall confidence in the verification"""
        if not self.verification_results:
            return 0.0
            
        confidences = [v['confidence'] for v in self.verification_results]
        return sum(confidences) / len(confidences)
        
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on verification"""
        recommendations = []
        
        # Based on findings
        if len(self.false_positives) > len(self.confirmed_bugs):
            recommendations.append("Add more detailed bug descriptions to improve verification")
            recommendations.append("Ensure tests run for realistic durations (>0.1s minimum)")
            
        if self.confirmed_bugs:
            # Group by type
            bug_types = {}
            for bug in self.confirmed_bugs:
                scenario = bug.get('scenario', 'Unknown')
                bug_types[scenario] = bug_types.get(scenario, 0) + 1
                
            for scenario, count in bug_types.items():
                recommendations.append(f"Focus debugging on {scenario} ({count} confirmed bugs)")
                
        # General recommendations
        recommendations.extend([
            "Add honeypot tests to validate bug detection",
            "Include detailed error messages in bug reports",
            "Run tests with actual module dependencies installed",
            "Add network activity monitoring to confirm real interactions"
        ])
        
        return recommendations


def generate_verification_report(verification_results: Dict[str, Any], output_file: str):
    """Generate a detailed verification report"""
    report = f"""# Critical Bug Verification Report

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}  
**Verification Confidence**: {verification_results['verification_confidence']:.1%}

## Summary

- **Total Bugs Claimed**: {verification_results['total_bugs_claimed']}
- **Verified Real Bugs**: {verification_results['verified_real_bugs']}
- **False Positives**: {verification_results['false_positives']}
- **Verification Confidence**: {verification_results['verification_confidence']:.1%}

## Skeptical Analysis

### Credibility Issues
"""
    
    for issue in verification_results['analysis']['credibility_issues']:
        report += f"- ⚠️ {issue}\n"
        
    report += "\n### Positive Findings\n"
    
    for finding in verification_results['analysis']['positive_findings']:
        report += f"- ✅ {finding}\n"
        
    report += f"\n### Test Quality Score: {verification_results['analysis']['test_quality_score']:.1%}\n"
    
    report += "\n## Recommendations\n\n"
    
    for i, rec in enumerate(verification_results['recommendations'], 1):
        report += f"{i}. {rec}\n"
        
    report += "\n## Verification Details\n\n"
    report += "| Bug Type | Claimed | Verified | False Positives | Confidence |\n"
    report += "|----------|---------|----------|-----------------|------------|\n"
    
    # Group by scenario
    scenarios = {}
    for result in verification_results.get('verification_results', []):
        scenario = result['bug'].get('scenario', 'Unknown')
        if scenario not in scenarios:
            scenarios[scenario] = {'claimed': 0, 'verified': 0, 'false': 0}
        
        scenarios[scenario]['claimed'] += 1
        if result['is_real']:
            scenarios[scenario]['verified'] += 1
        else:
            scenarios[scenario]['false'] += 1
            
    for scenario, stats in scenarios.items():
        confidence = stats['verified'] / stats['claimed'] if stats['claimed'] > 0 else 0
        report += f"| {scenario} | {stats['claimed']} | {stats['verified']} | {stats['false']} | {confidence:.1%} |\n"
        
    # Save report
    with open(output_file, 'w') as f:
        f.write(report)
        
    logger.info(f"📄 Verification report saved to: {output_file}")
    
    return report


def main():
    """Run critical verification on the latest bug report"""
    # Find the latest bug report
    reports = list(Path('.').glob('bug_hunt_report_*.json'))
    if not reports:
        logger.error("No bug reports found")
        return 1
        
    latest_report = max(reports, key=lambda p: p.stat().st_mtime)
    logger.info(f"Verifying latest report: {latest_report}")
    
    # Run verification
    verifier = CriticalBugVerifier()
    results = verifier.verify_bug_report(str(latest_report))
    
    # Generate report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f'critical_verification_{timestamp}.md'
    generate_verification_report(results, report_file)
    
    # Summary
    logger.info(f"""
🔍 Verification Complete
Claimed: {results['total_bugs_claimed']}
Verified: {results['verified_real_bugs']}
False Positives: {results['false_positives']}
Confidence: {results['verification_confidence']:.1%}
    """)
    
    # Exit code based on confidence
    if results['verification_confidence'] < 0.5:
        logger.warning("⚠️ Low confidence in bug report accuracy")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())