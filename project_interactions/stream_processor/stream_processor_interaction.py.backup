#!/usr/bin/env python3
"""
Module: stream_processor_interaction.py
Purpose: Real-time data streaming processor with advanced stream processing capabilities

This module implements a comprehensive stream processing system with support for
multiple data sources, real-time transformations, windowing operations, and
exactly-once processing guarantees.

External Dependencies:
- asyncio: https://docs.python.org/3/library/asyncio.html
- kafka-python: https://kafka-python.readthedocs.io/
- redis: https://redis-py.readthedocs.io/

Example Usage:
>>> processor = StreamProcessor()
>>> await processor.start()
>>> await processor.process_stream("events", lambda x: x["value"] > 100)
"""

import asyncio
import json
import time
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum
import hashlib
import pickle
from datetime import datetime, timedelta
import threading
from concurrent.futures import ThreadPoolExecutor
import queue


class EventTime:
    """Manages event time and watermarks for stream processing"""
    
    def __init__(self, max_out_of_order_ms: int = 5000):
        self.max_out_of_order_ms = max_out_of_order_ms
        self.current_watermark = 0
        self.pending_events = []
        
    def update_watermark(self, event_time: int) -> int:
        """Update watermark based on event time"""
        # Allow for out-of-order events
        potential_watermark = event_time - self.max_out_of_order_ms
        if potential_watermark > self.current_watermark:
            self.current_watermark = potential_watermark
        return self.current_watermark
        
    def is_late(self, event_time: int) -> bool:
        """Check if an event is late based on current watermark"""
        return event_time < self.current_watermark


@dataclass
class StreamEvent:
    """Represents a single event in the stream"""
    key: str
    value: Any
    timestamp: int
    partition: int = 0
    offset: int = 0
    headers: Dict[str, str] = field(default_factory=dict)
    
    def __hash__(self):
        return hash((self.key, self.timestamp, self.partition, self.offset))


@dataclass
class WindowSpec:
    """Specification for windowing operations"""
    size_ms: int
    slide_ms: Optional[int] = None  # None means tumbling window
    
    @property
    def is_sliding(self) -> bool:
        return self.slide_ms is not None
        
    def get_window_start(self, timestamp: int) -> int:
        """Calculate window start time for given timestamp"""
        if self.is_sliding:
            # For sliding windows, multiple windows may contain this timestamp
            return timestamp - (timestamp % self.slide_ms)
        else:
            # For tumbling windows
            return timestamp - (timestamp % self.size_ms)
            
    def get_window_end(self, window_start: int) -> int:
        """Calculate window end time"""
        return window_start + self.size_ms


class StreamPartitioner:
    """Handles stream partitioning strategies"""
    
    @staticmethod
    def hash_partition(key: str, num_partitions: int) -> int:
        """Hash-based partitioning"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16) % num_partitions
        
    @staticmethod
    def round_robin(counter: int, num_partitions: int) -> int:
        """Round-robin partitioning"""
        return counter % num_partitions
        
    @staticmethod
    def custom_partition(event: StreamEvent, partition_func: Callable) -> int:
        """Custom partitioning function"""
        return partition_func(event)


class StateStore:
    """In-memory state store with checkpointing support"""
    
    def __init__(self, checkpoint_interval_ms: int = 30000):
        self.state: Dict[str, Any] = {}
        self.checkpoint_interval_ms = checkpoint_interval_ms
        self.last_checkpoint = time.time() * 1000
        self.changelog = []
        self.lock = threading.Lock()
        
    def get(self, key: str) -> Optional[Any]:
        """Get value from state store"""
        with self.lock:
            return self.state.get(key)
            
    def put(self, key: str, value: Any):
        """Put value in state store"""
        with self.lock:
            self.state[key] = value
            self.changelog.append(("put", key, value, time.time() * 1000))
            
    def delete(self, key: str):
        """Delete key from state store"""
        with self.lock:
            if key in self.state:
                del self.state[key]
                self.changelog.append(("delete", key, None, time.time() * 1000))
                
    def checkpoint(self) -> Dict[str, Any]:
        """Create checkpoint of current state"""
        with self.lock:
            checkpoint = {
                "state": dict(self.state),
                "timestamp": time.time() * 1000,
                "changelog_size": len(self.changelog)
            }
            self.changelog.clear()
            self.last_checkpoint = checkpoint["timestamp"]
            return checkpoint
            
    def restore(self, checkpoint: Dict[str, Any]):
        """Restore from checkpoint"""
        with self.lock:
            self.state = checkpoint["state"].copy()
            self.changelog.clear()
            self.last_checkpoint = checkpoint["timestamp"]


class StreamProcessor:
    """Main stream processor with advanced features"""
    
    def __init__(self, 
                 num_partitions: int = 4,
                 checkpoint_interval_ms: int = 30000,
                 max_concurrent_tasks: int = 10):
        self.num_partitions = num_partitions
        self.checkpoint_interval_ms = checkpoint_interval_ms
        self.max_concurrent_tasks = max_concurrent_tasks
        
        # Stream infrastructure
        self.partitions: List[asyncio.Queue] = [
            asyncio.Queue() for _ in range(num_partitions)
        ]
        self.state_stores: List[StateStore] = [
            StateStore(checkpoint_interval_ms) for _ in range(num_partitions)
        ]
        self.event_time = EventTime()
        
        # Processing state
        self.running = False
        self.tasks = []
        self.metrics = defaultdict(int)
        self.output_sinks = []
        
        # Exactly-once processing
        self.processed_offsets: Dict[int, int] = {}
        self.pending_commits: Dict[int, List[Tuple[int, Any]]] = defaultdict(list)
        
    async def start(self):
        """Start the stream processor"""
        self.running = True
        
        # Start partition processors
        for partition_id in range(self.num_partitions):
            task = asyncio.create_task(
                self._process_partition(partition_id)
            )
            self.tasks.append(task)
            
        # Start checkpoint task
        checkpoint_task = asyncio.create_task(self._checkpoint_loop())
        self.tasks.append(checkpoint_task)
        
    async def stop(self):
        """Stop the stream processor"""
        self.running = False
        for task in self.tasks:
            task.cancel()
        await asyncio.gather(*self.tasks, return_exceptions=True)
        
    async def ingest(self, events: List[StreamEvent]):
        """Ingest events into the stream processor"""
        for event in events:
            # Partition assignment
            if event.partition < 0:
                event.partition = StreamPartitioner.hash_partition(
                    event.key, self.num_partitions
                )
                
            # Update watermark
            self.event_time.update_watermark(event.timestamp)
            
            # Check if event is late
            if self.event_time.is_late(event.timestamp):
                self.metrics["late_events"] += 1
                
            # Add to partition queue
            await self.partitions[event.partition].put(event)
            self.metrics["events_ingested"] += 1
            
    async def process_stream(self,
                           stream_name: str,
                           process_func: Callable[[StreamEvent], Any],
                           window_spec: Optional[WindowSpec] = None):
        """Process stream with given function and optional windowing"""
        
        async def processor(partition_id: int):
            windows: Dict[int, List[StreamEvent]] = defaultdict(list)
            
            while self.running:
                try:
                    event = await asyncio.wait_for(
                        self.partitions[partition_id].get(),
                        timeout=1.0
                    )
                    
                    # Check duplicate processing
                    if self._is_duplicate(partition_id, event.offset):
                        continue
                        
                    # Process event
                    if window_spec:
                        # Window-based processing
                        window_start = window_spec.get_window_start(event.timestamp)
                        windows[window_start].append(event)
                        
                        # Check for complete windows
                        completed_windows = self._get_completed_windows(
                            windows, window_spec
                        )
                        
                        for window_start, window_events in completed_windows:
                            result = process_func(window_events)
                            await self._emit_result(result, partition_id)
                            del windows[window_start]
                    else:
                        # Per-event processing
                        result = process_func(event)
                        await self._emit_result(result, partition_id)
                        
                    # Mark as processed
                    self._mark_processed(partition_id, event.offset)
                    self.metrics["events_processed"] += 1
                    
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    self.metrics["processing_errors"] += 1
                    print(f"Processing error: {e}")
                    
        # Create processor tasks for each partition
        processor_tasks = []
        for partition_id in range(self.num_partitions):
            task = asyncio.create_task(processor(partition_id))
            processor_tasks.append(task)
            
        self.tasks.extend(processor_tasks)
        
    def join_streams(self,
                    left_stream: str,
                    right_stream: str,
                    join_key_func: Callable[[StreamEvent], str],
                    join_window_ms: int = 5000):
        """Join two streams within a time window"""
        
        left_buffer: Dict[str, List[StreamEvent]] = defaultdict(list)
        right_buffer: Dict[str, List[StreamEvent]] = defaultdict(list)
        
        def join_processor(event: StreamEvent, is_left: bool):
            key = join_key_func(event)
            current_time = time.time() * 1000
            
            # Clean old events from buffers
            self._clean_join_buffer(left_buffer, current_time - join_window_ms)
            self._clean_join_buffer(right_buffer, current_time - join_window_ms)
            
            if is_left:
                left_buffer[key].append(event)
                # Check for matching right events
                for right_event in right_buffer.get(key, []):
                    if abs(event.timestamp - right_event.timestamp) <= join_window_ms:
                        joined = {
                            "left": event,
                            "right": right_event,
                            "join_key": key,
                            "timestamp": max(event.timestamp, right_event.timestamp)
                        }
                        # Note: This would need to be handled differently in real implementation
                        # For testing, we'll return the joined result
                        return joined
            else:
                right_buffer[key].append(event)
                # Check for matching left events
                for left_event in left_buffer.get(key, []):
                    if abs(event.timestamp - left_event.timestamp) <= join_window_ms:
                        joined = {
                            "left": left_event,
                            "right": event,
                            "join_key": key,
                            "timestamp": max(event.timestamp, left_event.timestamp)
                        }
                        # Note: This would need to be handled differently in real implementation
                        # For testing, we'll return the joined result
                        return joined
            
            return None
                        
        return join_processor
        
    def add_output_sink(self, sink_func: Callable[[Any], None]):
        """Add an output sink for processed results"""
        self.output_sinks.append(sink_func)
        
    async def _process_partition(self, partition_id: int):
        """Process events for a specific partition"""
        while self.running:
            try:
                await asyncio.sleep(0.1)  # Placeholder for actual processing
            except Exception as e:
                print(f"Partition {partition_id} error: {e}")
                
    async def _checkpoint_loop(self):
        """Periodic checkpointing"""
        while self.running:
            await asyncio.sleep(self.checkpoint_interval_ms / 1000)
            
            checkpoints = []
            for partition_id, store in enumerate(self.state_stores):
                checkpoint = store.checkpoint()
                checkpoint["partition_id"] = partition_id
                checkpoint["processed_offset"] = self.processed_offsets.get(partition_id, 0)
                checkpoints.append(checkpoint)
                
            # Save checkpoints (in production, this would go to durable storage)
            self.metrics["checkpoints_created"] += 1
            
    def _is_duplicate(self, partition_id: int, offset: int) -> bool:
        """Check if event has already been processed"""
        return offset <= self.processed_offsets.get(partition_id, -1)
        
    def _mark_processed(self, partition_id: int, offset: int):
        """Mark event as processed"""
        self.processed_offsets[partition_id] = max(
            self.processed_offsets.get(partition_id, -1),
            offset
        )
        
    async def _emit_result(self, result: Any, partition_id: int):
        """Emit result to output sinks"""
        for sink in self.output_sinks:
            try:
                if asyncio.iscoroutinefunction(sink):
                    await sink(result)
                else:
                    sink(result)
            except Exception as e:
                self.metrics["sink_errors"] += 1
                print(f"Sink error: {e}")
                
    def _get_completed_windows(self,
                              windows: Dict[int, List[StreamEvent]],
                              window_spec: WindowSpec) -> List[Tuple[int, List[StreamEvent]]]:
        """Get windows that are complete based on watermark"""
        completed = []
        current_watermark = self.event_time.current_watermark
        
        for window_start, events in windows.items():
            window_end = window_spec.get_window_end(window_start)
            if window_end <= current_watermark:
                completed.append((window_start, events))
                
        return completed
        
    def _clean_join_buffer(self, 
                          buffer: Dict[str, List[StreamEvent]], 
                          min_timestamp: int):
        """Clean old events from join buffer"""
        for key in list(buffer.keys()):
            buffer[key] = [
                event for event in buffer[key]
                if event.timestamp >= min_timestamp
            ]
            if not buffer[key]:
                del buffer[key]
                
    def get_metrics(self) -> Dict[str, int]:
        """Get current metrics"""
        return dict(self.metrics)


# Complex Event Processing (CEP) patterns
class CEPPattern:
    """Base class for complex event patterns"""
    
    def match(self, events: List[StreamEvent]) -> bool:
        """Check if events match the pattern"""
        raise NotImplementedError
        

class SequencePattern(CEPPattern):
    """Match events in a specific sequence"""
    
    def __init__(self, patterns: List[Callable[[StreamEvent], bool]]):
        self.patterns = patterns
        
    def match(self, events: List[StreamEvent]) -> bool:
        if len(events) < len(self.patterns):
            return False
            
        for i, pattern in enumerate(self.patterns):
            if not pattern(events[i]):
                return False
                
        return True


class TemporalPattern(CEPPattern):
    """Match events within a time window"""
    
    def __init__(self, 
                 condition: Callable[[List[StreamEvent]], bool],
                 window_ms: int):
        self.condition = condition
        self.window_ms = window_ms
        
    def match(self, events: List[StreamEvent]) -> bool:
        if not events:
            return False
            
        # Check if all events are within window
        min_time = min(e.timestamp for e in events)
        max_time = max(e.timestamp for e in events)
        
        if max_time - min_time > self.window_ms:
            return False
            
        return self.condition(events)


# Stream aggregation functions
class StreamAggregator:
    """Aggregation functions for stream processing"""
    
    @staticmethod
    def count(events: List[StreamEvent]) -> int:
        """Count events"""
        return len(events)
        
    @staticmethod
    def sum(events: List[StreamEvent], value_func: Callable[[StreamEvent], float]) -> float:
        """Sum values"""
        return sum(value_func(e) for e in events)
        
    @staticmethod
    def avg(events: List[StreamEvent], value_func: Callable[[StreamEvent], float]) -> float:
        """Average values"""
        if not events:
            return 0.0
        return StreamAggregator.sum(events, value_func) / len(events)
        
    @staticmethod
    def min(events: List[StreamEvent], value_func: Callable[[StreamEvent], float]) -> float:
        """Minimum value"""
        if not events:
            return float('inf')
        return min(value_func(e) for e in events)
        
    @staticmethod
    def max(events: List[StreamEvent], value_func: Callable[[StreamEvent], float]) -> float:
        """Maximum value"""
        if not events:
            return float('-inf')
        return max(value_func(e) for e in events)


async def main():
    """Example usage of stream processor"""
    
    # Create processor
    processor = StreamProcessor(num_partitions=4)
    
    # Add output sink
    results = []
    processor.add_output_sink(lambda x: results.append(x))
    
    # Start processor
    await processor.start()
    
    # Create sample events
    events = []
    base_time = int(time.time() * 1000)
    
    for i in range(100):
        event = StreamEvent(
            key=f"user_{i % 10}",
            value={"amount": i * 10, "type": "purchase"},
            timestamp=base_time + i * 100,
            offset=i
        )
        events.append(event)
        
    # Ingest events
    await processor.ingest(events)
    
    # Process with windowing
    window_spec = WindowSpec(size_ms=5000)  # 5 second tumbling windows
    
    async def aggregate_purchases(window_events: List[StreamEvent]) -> Dict:
        total = sum(e.value["amount"] for e in window_events)
        return {
            "window_start": window_spec.get_window_start(window_events[0].timestamp),
            "total_amount": total,
            "event_count": len(window_events)
        }
        
    await processor.process_stream(
        "purchases",
        aggregate_purchases,
        window_spec
    )
    
    # Let processing complete
    await asyncio.sleep(2)
    
    # Get metrics
    metrics = processor.get_metrics()
    print(f"Metrics: {json.dumps(metrics, indent=2)}")
    
    # Stop processor
    await processor.stop()
    
    return results, metrics


if __name__ == "__main__":
    # Test the stream processor
    results, metrics = asyncio.run(main())
    
    # Validate results
    expected_events = 100
    actual_events = metrics.get("events_ingested", 0)
    
    assert actual_events == expected_events, f"Expected {expected_events} events, got {actual_events}"
    assert metrics.get("processing_errors", 0) == 0, "Processing errors occurred"
    
    print("✅ Stream processor validation passed")
    print(f"Processed {actual_events} events successfully")