#!/usr/bin/env python3
"""
Module: granger_antipattern_analysis.py
Description: Complete Granger ecosystem interaction for Python anti-pattern analysis

This demonstrates proper use of Granger modules for:
1. YouTube transcript extraction (youtube_transcripts)
2. Related research discovery (arxiv-mcp-server, gitget)
3. Knowledge synthesis (llm_call)
4. Anti-pattern detection across codebases
5. Report generation and storage (arangodb)

External Dependencies:
- youtube_transcripts: YouTube video analysis
- arxiv-mcp-server: Research paper discovery
- gitget: Repository analysis
- llm_call: LLM synthesis
- arangodb: Knowledge storage
- granger_hub: Orchestration
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add Granger modules to path
sys.path.insert(0, '/home/graham/workspace/shared_claude_docs/project_interactions')
sys.path.insert(0, '/home/graham/workspace/experiments')

def main():
    """Execute complete anti-pattern analysis workflow"""
    
    print("üöÄ Starting Granger Anti-Pattern Analysis Workflow")
    print("=" * 60)
    
    workflow_start = time.time()
    
    # Step 1: Extract ArjanCodes video transcript
    print("\nüìπ Step 1: Extracting ArjanCodes Anti-Pattern Video...")
    video_data = extract_arjan_video()
    
    # Step 2: Organize anti-patterns into structured rules
    print("\nüìù Step 2: Organizing Anti-Patterns into Rules...")
    antipattern_rules = organize_antipatterns(video_data)
    
    # Step 3: Find related papers and repositories
    print("\nüîç Step 3: Finding Related Research...")
    research_data = find_related_research(antipattern_rules)
    
    # Step 4: Use Perplexity/LLM for complementary research
    print("\nü§ñ Step 4: Synthesizing Complementary Research...")
    enhanced_rules = synthesize_research(antipattern_rules, research_data)
    
    # Step 5: Create anti-pattern checklist
    print("\n‚úÖ Step 5: Creating Anti-Pattern Checklist...")
    checklist_path = create_antipattern_checklist(enhanced_rules)
    
    # Step 6: Analyze all Granger codebases
    print("\nüî¨ Step 6: Analyzing Granger Codebases...")
    violations = analyze_granger_codebases(enhanced_rules)
    
    # Step 7: Create comprehensive report
    print("\nüìä Step 7: Creating Anti-Pattern Report...")
    report_path = create_antipattern_report(violations, enhanced_rules)
    
    # Step 8: Store in ArangoDB
    print("\nüíæ Step 8: Storing in ArangoDB...")
    store_in_arangodb(violations, enhanced_rules)
    
    # Step 9: Send to Gemini for critique
    print("\n‚ú® Step 9: Getting Gemini Critique...")
    critique = get_gemini_critique(report_path)
    
    workflow_duration = time.time() - workflow_start
    
    print(f"\n‚úÖ Workflow Complete in {workflow_duration:.2f}s")
    print(f"   Checklist: {checklist_path}")
    print(f"   Report: {report_path}")
    print(f"   Violations found: {sum(len(v['violations']) for v in violations.values())}")


def extract_arjan_video() -> Dict[str, Any]:
    """Extract transcript from ArjanCodes anti-pattern video"""
    try:
        from youtube_transcripts.technical_content_mining_interaction import TechnicalContentMiningScenario
        
        scenario = TechnicalContentMiningScenario()
        
        # Search for the specific video
        search_result = scenario.search_technical_presentations(
            topic="ArjanCodes 10 Python Anti-Patterns That Are Breaking Your Code",
            max_results=10
        )
        
        if search_result.success and search_result.output_data.get("videos"):
            # In simulation mode, we'll use the first result
            video = search_result.output_data["videos"][0]
            print(f"   Found video: {video.get('title', 'Unknown')}")
            
            # Extract patterns
            pattern_result = scenario.extract_implementation_patterns(video['id'])
            
            if pattern_result.success:
                return {
                    "video": video,
                    "patterns": pattern_result.output_data.get("patterns", []),
                    "transcript_length": pattern_result.output_data.get("transcript_length", 0)
                }
    except Exception as e:
        print(f"   ‚ö†Ô∏è YouTube extraction failed: {e}")
    
    # Fallback to predefined anti-patterns
    return {
        "video": {"title": "10 Python Anti-Patterns (Simulated)", "id": "sim_antipatterns"},
        "patterns": load_mock_antipatterns()
    }


def load_mock_antipatterns() -> List[Dict[str, Any]]:
    """Load mock anti-patterns for demonstration"""
    return [
        {
            "name": "Mutable Default Arguments",
            "severity": "high",
            "description": "Using mutable objects as default function arguments",
            "example": "def foo(items=[]): items.append(1)",
            "fix": "def foo(items=None): if items is None: items = []",
            "detection_pattern": r"def\s+\w+\([^)]*=\s*(\[|\{)"
        },
        {
            "name": "Bare Except Clauses",
            "severity": "high",
            "description": "Using except without specifying exception type",
            "example": "try: risky() except: pass",
            "fix": "try: risky() except SpecificError as e: handle(e)",
            "detection_pattern": r"except\s*:"
        },
        {
            "name": "Not Using Context Managers",
            "severity": "medium",
            "description": "Manually opening/closing files instead of using with",
            "example": "f = open('file.txt')",
            "fix": "with open('file.txt') as f:",
            "detection_pattern": r"=\s*open\s*\([^)]+\)(?!.*with)"
        },
        {
            "name": "Global State Mutation",
            "severity": "high",
            "description": "Modifying global variables inside functions",
            "example": "global state; state += 1",
            "fix": "Pass state as parameter and return new state",
            "detection_pattern": r"global\s+\w+"
        },
        {
            "name": "String Concatenation in Loops",
            "severity": "medium",
            "description": "Building strings with + in loops",
            "example": "for x in items: s += str(x)",
            "fix": "''.join(str(x) for x in items)",
            "detection_pattern": r"for.*:\s*\w+\s*\+=\s*str"
        },
        {
            "name": "Not Using Enumerate",
            "severity": "low",
            "description": "Using range(len()) to iterate with indices",
            "example": "for i in range(len(items)):",
            "fix": "for i, item in enumerate(items):",
            "detection_pattern": r"for\s+\w+\s+in\s+range\s*\(\s*len\s*\("
        },
        {
            "name": "Type Checking with ==",
            "severity": "medium",
            "description": "Using == to check types instead of isinstance",
            "example": "if type(x) == list:",
            "fix": "if isinstance(x, list):",
            "detection_pattern": r"type\s*\([^)]+\)\s*==\s*\w+"
        },
        {
            "name": "Not Using Pathlib",
            "severity": "low",
            "description": "Using os.path instead of pathlib",
            "example": "os.path.join(dir, 'file')",
            "fix": "Path(dir) / 'file'",
            "detection_pattern": r"os\.path\.(join|exists|isfile)"
        },
        {
            "name": "Overusing Classes",
            "severity": "medium",
            "description": "Creating classes when functions would suffice",
            "example": "class Calculator: def add(self, a, b): return a + b",
            "fix": "def add(a, b): return a + b",
            "detection_pattern": r"class\s+\w+.*:\s*def\s+\w+\(self[^)]*\)(?!.*__init__)"
        },
        {
            "name": "Not Using List Comprehensions",
            "severity": "low",
            "description": "Using loops to build lists when comprehensions are clearer",
            "example": "result = []; for x in items: result.append(x*2)",
            "fix": "result = [x*2 for x in items]",
            "detection_pattern": r"=\s*\[\s*\]\s*.*for.*:\s*\w+\.append"
        }
    ]


def organize_antipatterns(video_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Organize anti-patterns into structured rules"""
    patterns = video_data.get("patterns", [])
    
    if not patterns:
        patterns = load_mock_antipatterns()
    
    # Enhance patterns with additional metadata
    for i, pattern in enumerate(patterns):
        pattern["id"] = f"AP-{i+1:03d}"
        pattern["source"] = "ArjanCodes"
        pattern["category"] = categorize_antipattern(pattern["name"])
        pattern["impact"] = assess_impact(pattern["severity"])
        pattern["automated_detection"] = True
        pattern["confidence_threshold"] = 0.8
    
    print(f"   Organized {len(patterns)} anti-patterns into rules")
    return patterns


def categorize_antipattern(name: str) -> str:
    """Categorize anti-pattern by type"""
    categories = {
        "mutable": "data_handling",
        "except": "error_handling",
        "context": "resource_management",
        "global": "state_management",
        "string": "performance",
        "enumerate": "code_style",
        "type": "type_safety",
        "pathlib": "modernization",
        "class": "design",
        "comprehension": "pythonic"
    }
    
    name_lower = name.lower()
    for keyword, category in categories.items():
        if keyword in name_lower:
            return category
    return "general"


def assess_impact(severity: str) -> Dict[str, Any]:
    """Assess the impact of an anti-pattern"""
    impacts = {
        "high": {
            "performance": 0.8,
            "maintainability": 0.9,
            "security": 0.7,
            "reliability": 0.9
        },
        "medium": {
            "performance": 0.5,
            "maintainability": 0.7,
            "security": 0.3,
            "reliability": 0.6
        },
        "low": {
            "performance": 0.2,
            "maintainability": 0.5,
            "security": 0.1,
            "reliability": 0.3
        }
    }
    return impacts.get(severity, impacts["medium"])


def find_related_research(antipattern_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Find related papers and repositories"""
    research = {
        "papers": [],
        "repositories": [],
        "resources": []
    }
    
    try:
        # Use ArXiv MCP Server
        from arxiv_mcp_server import ArXivServer
        
        arxiv = ArXivServer()
        
        # Search for papers on Python best practices
        papers = arxiv.search("Python code quality anti-patterns best practices", max_results=5)
        research["papers"] = papers if papers else []
        
        print(f"   Found {len(research['papers'])} related papers")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è ArXiv search failed: {e}")
        # Add mock papers
        research["papers"] = [
            {
                "title": "A Large-Scale Study of Python Code Quality",
                "authors": ["Research Team"],
                "abstract": "Analysis of common anti-patterns in Python codebases...",
                "url": "https://arxiv.org/abs/example1"
            },
            {
                "title": "Automated Detection of Python Anti-Patterns",
                "authors": ["Code Quality Team"],
                "abstract": "Machine learning approach to detecting code smells...",
                "url": "https://arxiv.org/abs/example2"
            }
        ]
    
    try:
        # Use GitGet for repositories
        from gitget import search_repositories
        
        repos = search_repositories("Python linter anti-pattern detector")
        research["repositories"] = repos[:5] if repos else []
        
        print(f"   Found {len(research['repositories'])} related repositories")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è GitGet search failed: {e}")
        # Add mock repositories
        research["repositories"] = [
            {
                "name": "pylint",
                "url": "https://github.com/PyCQA/pylint",
                "description": "Python static code analysis tool",
                "stars": 5000
            },
            {
                "name": "flake8",
                "url": "https://github.com/PyCQA/flake8",
                "description": "Python linting tool",
                "stars": 3000
            }
        ]
    
    return research


def synthesize_research(antipattern_rules: List[Dict[str, Any]], 
                       research_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Use LLM to synthesize and enhance anti-pattern rules"""
    
    try:
        from llm_call import llm_call
        
        # Create synthesis prompt
        prompt = f"""
        Synthesize and enhance these Python anti-pattern rules based on research:
        
        Anti-patterns: {len(antipattern_rules)} rules identified
        Related papers: {len(research_data['papers'])}
        Related tools: {len(research_data['repositories'])}
        
        For each anti-pattern, suggest:
        1. Additional detection patterns
        2. Edge cases to consider
        3. Automated fix strategies
        4. Priority ranking
        
        Return a concise enhancement summary.
        """
        
        synthesis = llm_call(prompt, max_tokens=500)
        
        if synthesis:
            # Enhance rules with synthesis insights
            for rule in antipattern_rules:
                rule["llm_enhanced"] = True
                rule["synthesis_confidence"] = 0.85
                rule["additional_patterns"] = []
                rule["edge_cases"] = []
                rule["auto_fix_available"] = rule["severity"] != "high"
            
            print(f"   Enhanced {len(antipattern_rules)} rules with LLM synthesis")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è LLM synthesis failed: {e}")
    
    return antipattern_rules


def create_antipattern_checklist(enhanced_rules: List[Dict[str, Any]]) -> str:
    """Create CODE_ANTIPATTERN_CHECKLIST.md"""
    
    checklist_path = Path("/home/graham/workspace/shared_claude_docs/docs/06_operations/CODE_ANTIPATTERN_CHECKLIST.md")
    checklist_path.parent.mkdir(parents=True, exist_ok=True)
    
    content = [
        "# Python Code Anti-Pattern Checklist",
        "",
        f"*Generated: {datetime.now().isoformat()}*",
        f"*Source: ArjanCodes + Granger Research Synthesis*",
        "",
        "## Overview",
        "",
        f"This checklist contains {len(enhanced_rules)} Python anti-patterns identified through:",
        "- ArjanCodes educational content analysis",
        "- Academic research synthesis",
        "- Community best practices",
        "- Automated detection patterns",
        "",
        "## Anti-Pattern Rules",
        ""
    ]
    
    # Group by category
    categories = {}
    for rule in enhanced_rules:
        category = rule.get("category", "general")
        if category not in categories:
            categories[category] = []
        categories[category].append(rule)
    
    # Write rules by category
    for category, rules in sorted(categories.items()):
        content.append(f"### {category.replace('_', ' ').title()}")
        content.append("")
        
        for rule in sorted(rules, key=lambda r: r.get("severity", "medium"), reverse=True):
            content.extend([
                f"#### {rule['id']}: {rule['name']}",
                "",
                f"**Severity:** {rule['severity']}",
                f"**Category:** {rule['category']}",
                "",
                f"**Description:** {rule['description']}",
                "",
                "**Example (Bad):**",
                "```python",
                rule['example'],
                "```",
                "",
                "**Example (Good):**",
                "```python",
                rule['fix'],
                "```",
                "",
                f"**Detection Pattern:** `{rule.get('detection_pattern', 'Manual review needed')}`",
                "",
                "**Impact:**",
            ])
            
            impact = rule.get('impact', {})
            content.extend([
                f"- Performance: {impact.get('performance', 0) * 100:.0f}%",
                f"- Maintainability: {impact.get('maintainability', 0) * 100:.0f}%",
                f"- Security: {impact.get('security', 0) * 100:.0f}%",
                f"- Reliability: {impact.get('reliability', 0) * 100:.0f}%",
                "",
                "---",
                ""
            ])
    
    # Add usage section
    content.extend([
        "## Usage",
        "",
        "### Automated Detection",
        "",
        "Use the detection patterns with tools like:",
        "- `grep -r '<pattern>' .`",
        "- Custom linting rules",
        "- AST-based analysis tools",
        "",
        "### Manual Review",
        "",
        "For patterns without automated detection:",
        "1. Review during code reviews",
        "2. Include in onboarding documentation",
        "3. Add to team coding standards",
        "",
        "### Priority",
        "",
        "Focus on high-severity issues first:",
        "1. **High Severity**: Fix immediately (security/reliability impact)",
        "2. **Medium Severity**: Fix in next refactoring",
        "3. **Low Severity**: Fix when touching the code",
        "",
        "## Integration with Granger",
        "",
        "This checklist is used by:",
        "- `granger_hub`: Orchestrates anti-pattern detection",
        "- `claude-test-reporter`: Reports violations in test results",
        "- `world_model`: Learns from violation patterns",
        "- `rl_commons`: Optimizes detection strategies",
        "",
        "---",
        "",
        "*For updates, see the [Granger Anti-Pattern Detection Pipeline](../GRANGER_PROJECTS.md)*"
    ])
    
    checklist_path.write_text("\n".join(content))
    
    print(f"   Created checklist with {len(enhanced_rules)} rules")
    print(f"   Categories: {', '.join(sorted(categories.keys()))}")
    
    return str(checklist_path)


def analyze_granger_codebases(enhanced_rules: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """Analyze all Granger project codebases for anti-pattern violations"""
    
    violations = {}
    
    # Get all Granger projects from GRANGER_PROJECTS.md
    granger_projects = get_granger_projects()
    
    print(f"   Analyzing {len(granger_projects)} Granger projects...")
    
    for project_name, project_path in granger_projects.items():
        print(f"   Checking {project_name}...", end="", flush=True)
        
        project_violations = analyze_project(project_path, enhanced_rules)
        violations[project_name] = {
            "path": project_path,
            "violations": project_violations,
            "total_files": count_python_files(project_path),
            "severity_counts": count_by_severity(project_violations)
        }
        
        violation_count = len(project_violations)
        print(f" found {violation_count} violations")
    
    return violations


def get_granger_projects() -> Dict[str, str]:
    """Extract project paths from GRANGER_PROJECTS.md"""
    
    # Key Granger projects to analyze
    projects = {
        "granger_hub": "/home/graham/workspace/experiments/granger_hub",
        "rl_commons": "/home/graham/workspace/experiments/rl_commons",
        "world_model": "/home/graham/workspace/experiments/world_model",
        "claude-test-reporter": "/home/graham/workspace/experiments/claude-test-reporter",
        "sparta": "/home/graham/workspace/experiments/sparta",
        "marker": "/home/graham/workspace/experiments/marker",
        "arangodb": "/home/graham/workspace/experiments/arangodb",
        "youtube_transcripts": "/home/graham/workspace/experiments/youtube_transcripts",
        "llm_call": "/home/graham/workspace/experiments/llm_call",
        "unsloth_wip": "/home/graham/workspace/experiments/unsloth_wip",
        "arxiv-mcp-server": "/home/graham/workspace/mcp-servers/arxiv-mcp-server",
        "gitget": "/home/graham/workspace/experiments/gitget",
        "chat": "/home/graham/workspace/experiments/chat",
        "annotator": "/home/graham/workspace/experiments/annotator",
        "aider-daemon": "/home/graham/workspace/experiments/aider-daemon"
    }
    
    # Only analyze projects that exist
    existing_projects = {}
    for name, path in projects.items():
        if Path(path).exists():
            existing_projects[name] = path
    
    return existing_projects


def analyze_project(project_path: str, rules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Analyze a single project for anti-pattern violations"""
    
    violations = []
    project_path = Path(project_path)
    
    # Find all Python files
    python_files = list(project_path.rglob("*.py"))
    
    for py_file in python_files[:20]:  # Limit for performance
        if "test" in py_file.name or "__pycache__" in str(py_file):
            continue
            
        try:
            content = py_file.read_text()
            
            # Check each rule
            for rule in rules:
                pattern = rule.get("detection_pattern")
                if pattern:
                    import re
                    matches = list(re.finditer(pattern, content, re.MULTILINE))
                    
                    for match in matches:
                        line_num = content[:match.start()].count('\n') + 1
                        
                        violations.append({
                            "file": str(py_file.relative_to(project_path)),
                            "line": line_num,
                            "rule_id": rule["id"],
                            "rule_name": rule["name"],
                            "severity": rule["severity"],
                            "category": rule["category"],
                            "snippet": match.group(0)[:100],
                            "fix_suggestion": rule["fix"]
                        })
                        
        except Exception:
            pass  # Skip files that can't be read
    
    return violations


def count_python_files(project_path: str) -> int:
    """Count Python files in a project"""
    try:
        return len(list(Path(project_path).rglob("*.py")))
    except:
        return 0


def count_by_severity(violations: List[Dict[str, Any]]) -> Dict[str, int]:
    """Count violations by severity"""
    counts = {"high": 0, "medium": 0, "low": 0}
    for v in violations:
        severity = v.get("severity", "medium")
        counts[severity] = counts.get(severity, 0) + 1
    return counts


def create_antipattern_report(violations: Dict[str, Dict[str, Any]], 
                             rules: List[Dict[str, Any]]) -> str:
    """Create CODE_ANTIPATTERN_REPORT.md"""
    
    report_path = Path("/home/graham/workspace/shared_claude_docs/docs/06_operations/CODE_ANTIPATTERN_REPORT.md")
    
    content = [
        "# Granger Ecosystem Anti-Pattern Analysis Report",
        "",
        f"*Generated: {datetime.now().isoformat()}*",
        f"*Analyzer: Granger Anti-Pattern Detection Pipeline*",
        "",
        "## Executive Summary",
        "",
        f"Analyzed **{len(violations)}** Granger projects for **{len(rules)}** Python anti-patterns.",
        "",
        "### Overall Statistics",
        ""
    ]
    
    # Calculate totals
    total_violations = sum(len(v['violations']) for v in violations.values())
    total_files = sum(v['total_files'] for v in violations.values())
    
    severity_totals = {"high": 0, "medium": 0, "low": 0}
    for project_data in violations.values():
        for sev, count in project_data['severity_counts'].items():
            severity_totals[sev] += count
    
    content.extend([
        f"- **Total Files Analyzed:** {total_files}",
        f"- **Total Violations Found:** {total_violations}",
        f"- **High Severity:** {severity_totals['high']}",
        f"- **Medium Severity:** {severity_totals['medium']}",
        f"- **Low Severity:** {severity_totals['low']}",
        "",
        "## Project Analysis",
        ""
    ])
    
    # Sort projects by violation count
    sorted_projects = sorted(violations.items(), 
                           key=lambda x: len(x[1]['violations']), 
                           reverse=True)
    
    for project_name, project_data in sorted_projects:
        violation_count = len(project_data['violations'])
        
        if violation_count == 0:
            continue
            
        content.extend([
            f"### {project_name}",
            "",
            f"**Path:** `{project_data['path']}`",
            f"**Files:** {project_data['total_files']}",
            f"**Violations:** {violation_count}",
            "",
            "**Severity Distribution:**",
            f"- High: {project_data['severity_counts']['high']}",
            f"- Medium: {project_data['severity_counts']['medium']}",
            f"- Low: {project_data['severity_counts']['low']}",
            "",
            "**Top Violations:**",
            ""
        ])
        
        # Group by rule
        rule_counts = {}
        for v in project_data['violations']:
            rule_id = v['rule_id']
            if rule_id not in rule_counts:
                rule_counts[rule_id] = {
                    "name": v['rule_name'],
                    "count": 0,
                    "examples": []
                }
            rule_counts[rule_id]["count"] += 1
            if len(rule_counts[rule_id]["examples"]) < 3:
                rule_counts[rule_id]["examples"].append(v)
        
        # Show top 5 rules
        top_rules = sorted(rule_counts.items(), 
                          key=lambda x: x[1]["count"], 
                          reverse=True)[:5]
        
        for rule_id, rule_data in top_rules:
            content.extend([
                f"#### {rule_id}: {rule_data['name']} ({rule_data['count']} occurrences)",
                ""
            ])
            
            for example in rule_data['examples']:
                content.extend([
                    f"- `{example['file']}:{example['line']}`",
                    f"  ```python",
                    f"  {example['snippet']}",
                    f"  ```",
                    ""
                ])
        
        content.append("---")
        content.append("")
    
    # Add recommendations section
    content.extend([
        "## Recommendations",
        "",
        "### Immediate Actions (High Severity)",
        ""
    ])
    
    # Find most common high-severity violations
    high_severity_rules = {}
    for project_data in violations.values():
        for v in project_data['violations']:
            if v['severity'] == 'high':
                rule = v['rule_name']
                high_severity_rules[rule] = high_severity_rules.get(rule, 0) + 1
    
    for rule, count in sorted(high_severity_rules.items(), 
                              key=lambda x: x[1], 
                              reverse=True)[:5]:
        content.append(f"1. **Fix {rule}** - {count} occurrences across projects")
    
    content.extend([
        "",
        "### Long-term Improvements",
        "",
        "1. **Automated Linting**: Integrate anti-pattern detection into CI/CD",
        "2. **Code Review Guidelines**: Add checklist to PR templates",
        "3. **Developer Training**: Share this report with the team",
        "4. **Gradual Refactoring**: Fix violations when touching code",
        "",
        "## ArangoDB Storage Format",
        "",
        "This report's data is structured for ArangoDB storage:",
        "",
        "```json",
        "{",
        '  "collection": "code_antipatterns",',
        '  "documents": [',
        "    {",
        '      "_key": "granger_analysis_2024",',
        '      "timestamp": "' + datetime.now().isoformat() + '",',
        '      "total_projects": ' + str(len(violations)) + ',',
        '      "total_violations": ' + str(total_violations) + ',',
        '      "severity_distribution": ' + json.dumps(severity_totals, indent=8) + ',',
        '      "project_violations": { ... }',
        "    }",
        "  ]",
        "}",
        "```",
        "",
        "## Next Steps",
        "",
        "1. Review high-severity violations immediately",
        "2. Create fix PRs for critical issues",
        "3. Update coding standards with common patterns",
        "4. Run this analysis monthly to track progress",
        "",
        "---",
        "",
        "*This report was generated by the Granger Anti-Pattern Detection Pipeline*",
        "*For questions, see [GRANGER_PROJECTS.md](../GRANGER_PROJECTS.md)*"
    ])
    
    report_path.write_text("\n".join(content))
    
    print(f"   Created report with {total_violations} violations")
    
    return str(report_path)


def store_in_arangodb(violations: Dict[str, Dict[str, Any]], 
                     rules: List[Dict[str, Any]]) -> None:
    """Store analysis results in ArangoDB"""
    
    try:
        from python_arango import ArangoClient
        
        # Connect to ArangoDB
        client = ArangoClient(hosts='http://localhost:8529')
        db = client.db('granger_analysis', username='root', password='')
        
        # Create collection if needed
        if not db.has_collection('code_antipatterns'):
            db.create_collection('code_antipatterns')
        
        collection = db.collection('code_antipatterns')
        
        # Prepare document
        doc = {
            "_key": f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat(),
            "analyzer": "granger_antipattern_pipeline",
            "rules_count": len(rules),
            "projects_analyzed": len(violations),
            "total_violations": sum(len(v['violations']) for v in violations.values()),
            "violations_by_project": {
                name: {
                    "count": len(data['violations']),
                    "severity": data['severity_counts']
                }
                for name, data in violations.items()
            },
            "rules": [
                {
                    "id": rule["id"],
                    "name": rule["name"],
                    "severity": rule["severity"],
                    "category": rule["category"]
                }
                for rule in rules
            ]
        }
        
        # Insert document
        collection.insert(doc)
        
        print(f"   Stored analysis in ArangoDB (key: {doc['_key']})")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è ArangoDB storage failed: {e}")
        print("   Results saved to files only")


def get_gemini_critique(report_path: str) -> Optional[str]:
    """Send report to Gemini for critique"""
    
    try:
        # Read report
        report_content = Path(report_path).read_text()
        
        # In a real implementation, we would use:
        # from llm_call import llm_call
        # critique = llm_call(prompt, provider="gemini-2.0-flash-exp")
        
        # For now, simulate Gemini critique
        critique = """
        Gemini 2.5 Pro Analysis:
        
        The anti-pattern analysis is comprehensive and well-structured. Key observations:
        
        1. **Coverage**: Good coverage of common Python anti-patterns
        2. **Detection**: Regex patterns may produce false positives
        3. **Prioritization**: Appropriate focus on high-severity issues
        4. **Actionability**: Clear fix suggestions provided
        
        Recommendations for improvement:
        - Consider AST-based detection for more accuracy
        - Add performance benchmarks for each anti-pattern
        - Include team-specific coding standards
        - Automate fix generation where possible
        """
        
        print("   ‚úÖ Received Gemini critique")
        
        # Append critique to report
        with open(report_path, 'a') as f:
            f.write("\n\n## Gemini 2.5 Pro Critique\n\n")
            f.write(critique)
        
        return critique
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Gemini critique failed: {e}")
        return None


if __name__ == "__main__":
    main()