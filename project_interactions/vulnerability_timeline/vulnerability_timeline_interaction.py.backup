"""
Module: vulnerability_timeline_interaction.py
Purpose: Analyze security vulnerabilities across time from multiple sources in parallel

External Dependencies:
- asyncio: https://docs.python.org/3/library/asyncio.html
- datetime: https://docs.python.org/3/library/datetime.html
- collections: https://docs.python.org/3/library/collections.html
- json: https://docs.python.org/3/library/json.html
- statistics: https://docs.python.org/3/library/statistics.html
- typing: https://docs.python.org/3/library/typing.html

Example Usage:
>>> analyzer = VulnerabilityTimelineAnalyzer()
>>> results = await analyzer.analyze_vulnerabilities(
...     sources=['cve', 'nvd', 'security_advisories'],
...     start_date='2023-01-01',
...     end_date='2024-12-31'
... )
>>> print(f"Analyzed {results['total_vulnerabilities']} vulnerabilities")
'Analyzed 15432 vulnerabilities'
"""

import asyncio
import json
import random
import time
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from enum import Enum
import statistics


class VulnerabilitySource(Enum):
    """Enumeration of vulnerability data sources"""
    CVE = "cve"
    NVD = "nvd"
    SECURITY_ADVISORIES = "security_advisories"
    EXPLOIT_DB = "exploit_db"
    VENDOR_BULLETINS = "vendor_bulletins"


class ThreatVector(Enum):
    """Common threat vectors for vulnerabilities"""
    NETWORK = "network"
    LOCAL = "local"
    ADJACENT_NETWORK = "adjacent_network"
    PHYSICAL = "physical"


@dataclass
class Vulnerability:
    """Data class representing a security vulnerability"""
    id: str
    source: VulnerabilitySource
    published_date: datetime
    modified_date: datetime
    severity: float  # CVSS score 0-10
    threat_vector: ThreatVector
    affected_products: List[str]
    description: str
    exploit_available: bool
    patch_available: bool
    references: List[str]


@dataclass
class TemporalPattern:
    """Data class for temporal vulnerability patterns"""
    period: str  # e.g., "2024-Q1", "2024-W15"
    count: int
    avg_severity: float
    critical_count: int
    exploited_count: int
    patched_percentage: float
    dominant_vector: ThreatVector
    emerging_products: List[str]


@dataclass
class TrendAnalysis:
    """Data class for vulnerability trend analysis"""
    trend_type: str  # "increasing", "decreasing", "stable", "cyclical"
    confidence: float  # 0-1
    slope: float  # rate of change
    seasonality_detected: bool
    peak_periods: List[str]
    prediction_window: int  # days
    predicted_values: List[float]


class VulnerabilityTimelineAnalyzer:
    """Analyzes security vulnerabilities across time from multiple sources"""
    
    def __init__(self):
        self.vulnerabilities: List[Vulnerability] = []
        self.temporal_patterns: Dict[str, TemporalPattern] = {}
        self.trend_analyses: Dict[str, TrendAnalysis] = {}
        self.source_parsers = {
            VulnerabilitySource.CVE: self._parse_cve_data,
            VulnerabilitySource.NVD: self._parse_nvd_data,
            VulnerabilitySource.SECURITY_ADVISORIES: self._parse_advisory_data,
            VulnerabilitySource.EXPLOIT_DB: self._parse_exploit_db,
            VulnerabilitySource.VENDOR_BULLETINS: self._parse_vendor_bulletins
        }
    
    async def analyze_vulnerabilities(
        self,
        sources: List[str],
        start_date: str,
        end_date: str,
        parallel_fetch: bool = True
    ) -> Dict[str, Any]:
        """
        Main method to analyze vulnerabilities from multiple sources
        
        Args:
            sources: List of vulnerability sources to analyze
            start_date: Start date for analysis (YYYY-MM-DD)
            end_date: End date for analysis (YYYY-MM-DD)
            parallel_fetch: Whether to fetch from sources in parallel
            
        Returns:
            Dictionary containing analysis results
        """
        start_time = time.time()
        
        # Convert dates
        start_dt = datetime.fromisoformat(start_date)
        end_dt = datetime.fromisoformat(end_date)
        
        # Parse vulnerability data from sources
        if parallel_fetch:
            await self._fetch_parallel(sources, start_dt, end_dt)
        else:
            await self._fetch_sequential(sources, start_dt, end_dt)
        
        # Analyze temporal patterns
        self._analyze_temporal_patterns(start_dt, end_dt)
        
        # Detect trends
        self._detect_trends()
        
        # Predict future patterns
        predictions = self._predict_future_patterns()
        
        # Generate visualizations
        visualizations = self._generate_visualizations()
        
        # Compile results
        results = {
            "total_vulnerabilities": len(self.vulnerabilities),
            "sources_analyzed": sources,
            "date_range": {
                "start": start_date,
                "end": end_date
            },
            "temporal_patterns": self._serialize_patterns(),
            "trend_analyses": self._serialize_trends(),
            "predictions": predictions,
            "visualizations": visualizations,
            "emerging_threats": self._identify_emerging_threats(),
            "statistics": self._calculate_statistics(),
            "analysis_duration": time.time() - start_time
        }
        
        return results
    
    async def _fetch_parallel(
        self,
        sources: List[str],
        start_date: datetime,
        end_date: datetime
    ) -> None:
        """Fetch vulnerability data from multiple sources in parallel"""
        tasks = []
        for source_str in sources:
            source = VulnerabilitySource(source_str)
            if source in self.source_parsers:
                task = self.source_parsers[source](start_date, end_date)
                tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        for vulns in results:
            self.vulnerabilities.extend(vulns)
    
    async def _fetch_sequential(
        self,
        sources: List[str],
        start_date: datetime,
        end_date: datetime
    ) -> None:
        """Fetch vulnerability data from sources sequentially"""
        for source_str in sources:
            source = VulnerabilitySource(source_str)
            if source in self.source_parsers:
                vulns = await self.source_parsers[source](start_date, end_date)
                self.vulnerabilities.extend(vulns)
    
    async def _parse_cve_data(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Vulnerability]:
        """Parse CVE data (simulated)"""
        await asyncio.sleep(0.1)  # Simulate API call
        
        vulnerabilities = []
        days = (end_date - start_date).days
        
        for i in range(random.randint(1000, 2000)):
            pub_date = start_date + timedelta(days=random.randint(0, days))
            vuln = Vulnerability(
                id=f"CVE-{pub_date.year}-{random.randint(1000, 9999)}",
                source=VulnerabilitySource.CVE,
                published_date=pub_date,
                modified_date=pub_date + timedelta(days=random.randint(0, 30)),
                severity=random.uniform(0, 10),
                threat_vector=random.choice(list(ThreatVector)),
                affected_products=[f"Product-{random.randint(1, 100)}" 
                                 for _ in range(random.randint(1, 5))],
                description=f"CVE vulnerability affecting multiple products",
                exploit_available=random.random() < 0.2,
                patch_available=random.random() < 0.7,
                references=[f"https://cve.mitre.org/cgi-bin/cvename.cgi?name={i}"]
            )
            vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    async def _parse_nvd_data(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Vulnerability]:
        """Parse NVD data (simulated)"""
        await asyncio.sleep(0.15)  # Simulate API call
        
        vulnerabilities = []
        days = (end_date - start_date).days
        
        for i in range(random.randint(800, 1500)):
            pub_date = start_date + timedelta(days=random.randint(0, days))
            vuln = Vulnerability(
                id=f"NVD-{pub_date.year}-{random.randint(1000, 9999)}",
                source=VulnerabilitySource.NVD,
                published_date=pub_date,
                modified_date=pub_date + timedelta(days=random.randint(0, 45)),
                severity=random.uniform(0, 10),
                threat_vector=random.choice(list(ThreatVector)),
                affected_products=[f"Software-{random.randint(1, 150)}" 
                                 for _ in range(random.randint(1, 3))],
                description=f"NVD analyzed vulnerability with CVSS scoring",
                exploit_available=random.random() < 0.15,
                patch_available=random.random() < 0.8,
                references=[f"https://nvd.nist.gov/vuln/detail/{i}"]
            )
            vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    async def _parse_advisory_data(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Vulnerability]:
        """Parse security advisory data (simulated)"""
        await asyncio.sleep(0.1)  # Simulate API call
        
        vulnerabilities = []
        days = (end_date - start_date).days
        
        for i in range(random.randint(500, 1000)):
            pub_date = start_date + timedelta(days=random.randint(0, days))
            vuln = Vulnerability(
                id=f"SA-{pub_date.year}-{random.randint(100, 999)}",
                source=VulnerabilitySource.SECURITY_ADVISORIES,
                published_date=pub_date,
                modified_date=pub_date + timedelta(days=random.randint(0, 20)),
                severity=random.uniform(3, 10),  # Advisories often for higher severity
                threat_vector=random.choice(list(ThreatVector)),
                affected_products=[f"Enterprise-{random.randint(1, 50)}" 
                                 for _ in range(random.randint(1, 4))],
                description=f"Security advisory for enterprise software",
                exploit_available=random.random() < 0.25,
                patch_available=random.random() < 0.9,
                references=[f"https://securityadvisory.example.com/{i}"]
            )
            vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    async def _parse_exploit_db(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Vulnerability]:
        """Parse Exploit DB data (simulated)"""
        await asyncio.sleep(0.08)  # Simulate API call
        
        vulnerabilities = []
        days = (end_date - start_date).days
        
        for i in range(random.randint(300, 600)):
            pub_date = start_date + timedelta(days=random.randint(0, days))
            vuln = Vulnerability(
                id=f"EDB-{random.randint(40000, 50000)}",
                source=VulnerabilitySource.EXPLOIT_DB,
                published_date=pub_date,
                modified_date=pub_date,
                severity=random.uniform(5, 10),  # Exploits are usually serious
                threat_vector=random.choice([ThreatVector.NETWORK, ThreatVector.LOCAL]),
                affected_products=[f"App-{random.randint(1, 200)}" 
                                 for _ in range(random.randint(1, 2))],
                description=f"Exploit available for vulnerability",
                exploit_available=True,  # By definition
                patch_available=random.random() < 0.5,
                references=[f"https://exploit-db.com/exploits/{i}"]
            )
            vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    async def _parse_vendor_bulletins(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Vulnerability]:
        """Parse vendor security bulletins (simulated)"""
        await asyncio.sleep(0.12)  # Simulate API call
        
        vulnerabilities = []
        days = (end_date - start_date).days
        vendors = ["Microsoft", "Apple", "Google", "Oracle", "Adobe", "Cisco"]
        
        for i in range(random.randint(400, 800)):
            pub_date = start_date + timedelta(days=random.randint(0, days))
            vendor = random.choice(vendors)
            vuln = Vulnerability(
                id=f"{vendor}-{pub_date.year}-{random.randint(100, 999)}",
                source=VulnerabilitySource.VENDOR_BULLETINS,
                published_date=pub_date,
                modified_date=pub_date + timedelta(days=random.randint(0, 15)),
                severity=random.uniform(2, 10),
                threat_vector=random.choice(list(ThreatVector)),
                affected_products=[f"{vendor}-Product-{random.randint(1, 30)}" 
                                 for _ in range(random.randint(1, 3))],
                description=f"{vendor} security bulletin",
                exploit_available=random.random() < 0.1,
                patch_available=random.random() < 0.95,  # Vendors usually provide patches
                references=[f"https://{vendor.lower()}.com/security/{i}"]
            )
            vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    def _analyze_temporal_patterns(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> None:
        """Analyze temporal patterns in vulnerability data"""
        # Group by different time periods
        monthly_groups = defaultdict(list)
        weekly_groups = defaultdict(list)
        quarterly_groups = defaultdict(list)
        
        for vuln in self.vulnerabilities:
            # Monthly
            month_key = vuln.published_date.strftime("%Y-%m")
            monthly_groups[month_key].append(vuln)
            
            # Weekly
            week_key = vuln.published_date.strftime("%Y-W%U")
            weekly_groups[week_key].append(vuln)
            
            # Quarterly
            quarter = (vuln.published_date.month - 1) // 3 + 1
            quarter_key = f"{vuln.published_date.year}-Q{quarter}"
            quarterly_groups[quarter_key].append(vuln)
        
        # Analyze each temporal grouping
        for period_type, groups in [
            ("monthly", monthly_groups),
            ("weekly", weekly_groups),
            ("quarterly", quarterly_groups)
        ]:
            for period, vulns in groups.items():
                pattern = self._create_temporal_pattern(period, vulns)
                self.temporal_patterns[f"{period_type}_{period}"] = pattern
    
    def _create_temporal_pattern(
        self,
        period: str,
        vulnerabilities: List[Vulnerability]
    ) -> TemporalPattern:
        """Create a temporal pattern from a group of vulnerabilities"""
        severities = [v.severity for v in vulnerabilities]
        critical_count = sum(1 for v in vulnerabilities if v.severity >= 9.0)
        exploited_count = sum(1 for v in vulnerabilities if v.exploit_available)
        patched_count = sum(1 for v in vulnerabilities if v.patch_available)
        
        # Find dominant threat vector
        vector_counts = Counter(v.threat_vector for v in vulnerabilities)
        dominant_vector = vector_counts.most_common(1)[0][0] if vector_counts else ThreatVector.NETWORK
        
        # Find emerging products (appearing frequently)
        all_products = []
        for v in vulnerabilities:
            all_products.extend(v.affected_products)
        product_counts = Counter(all_products)
        emerging_products = [p for p, c in product_counts.most_common(5)]
        
        return TemporalPattern(
            period=period,
            count=len(vulnerabilities),
            avg_severity=statistics.mean(severities) if severities else 0,
            critical_count=critical_count,
            exploited_count=exploited_count,
            patched_percentage=(patched_count / len(vulnerabilities) * 100) if vulnerabilities else 0,
            dominant_vector=dominant_vector,
            emerging_products=emerging_products
        )
    
    def _detect_trends(self) -> None:
        """Detect trends in vulnerability patterns"""
        # Analyze monthly trends
        monthly_patterns = sorted(
            [(k, v) for k, v in self.temporal_patterns.items() if k.startswith("monthly_")],
            key=lambda x: x[0]
        )
        
        if len(monthly_patterns) >= 3:
            # Vulnerability count trend
            counts = [p[1].count for p in monthly_patterns]
            count_trend = self._analyze_trend_series(counts, "vulnerability_count")
            self.trend_analyses["count_trend"] = count_trend
            
            # Severity trend
            severities = [p[1].avg_severity for p in monthly_patterns]
            severity_trend = self._analyze_trend_series(severities, "severity")
            self.trend_analyses["severity_trend"] = severity_trend
            
            # Exploit availability trend
            exploit_rates = [p[1].exploited_count / p[1].count * 100 
                           for p in monthly_patterns if p[1].count > 0]
            if exploit_rates:
                exploit_trend = self._analyze_trend_series(exploit_rates, "exploit_rate")
                self.trend_analyses["exploit_trend"] = exploit_trend
    
    def _analyze_trend_series(
        self,
        values: List[float],
        trend_name: str
    ) -> TrendAnalysis:
        """Analyze a time series for trends"""
        if len(values) < 2:
            return TrendAnalysis(
                trend_type="insufficient_data",
                confidence=0,
                slope=0,
                seasonality_detected=False,
                peak_periods=[],
                prediction_window=0,
                predicted_values=[]
            )
        
        # Calculate slope (simple linear regression)
        n = len(values)
        x_values = list(range(n))
        x_mean = sum(x_values) / n
        y_mean = sum(values) / n
        
        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, values))
        denominator = sum((x - x_mean) ** 2 for x in x_values)
        
        slope = numerator / denominator if denominator != 0 else 0
        
        # Determine trend type
        if abs(slope) < 0.1:
            trend_type = "stable"
        elif slope > 0:
            trend_type = "increasing"
        else:
            trend_type = "decreasing"
        
        # Simple seasonality detection (look for repeating patterns)
        seasonality_detected = False
        peak_periods = []
        
        if len(values) >= 12:  # Need at least a year of data
            # Check for quarterly patterns
            quarterly_avgs = []
            for i in range(0, len(values) - 3, 3):
                quarterly_avgs.append(sum(values[i:i+3]) / 3)
            
            if len(quarterly_avgs) >= 4:
                # Simple check: do Q1 values tend to be similar?
                q1_values = quarterly_avgs[::4]
                if len(q1_values) >= 2:
                    variance = statistics.variance(q1_values) if len(q1_values) > 1 else 0
                    if variance < statistics.variance(quarterly_avgs) * 0.5:
                        seasonality_detected = True
        
        # Find peaks
        for i in range(1, len(values) - 1):
            if values[i] > values[i-1] and values[i] > values[i+1]:
                peak_periods.append(f"Period_{i}")
        
        # Simple prediction (linear extrapolation)
        prediction_window = min(6, len(values) // 2)  # Predict up to 6 periods
        predicted_values = []
        for i in range(1, prediction_window + 1):
            predicted = y_mean + slope * (n + i - x_mean)
            predicted_values.append(max(0, predicted))  # Ensure non-negative
        
        # Calculate confidence (based on R-squared)
        ss_tot = sum((y - y_mean) ** 2 for y in values)
        ss_res = sum((y - (y_mean + slope * (x - x_mean))) ** 2 
                    for x, y in zip(x_values, values))
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        confidence = max(0, min(1, r_squared))
        
        return TrendAnalysis(
            trend_type=trend_type,
            confidence=confidence,
            slope=slope,
            seasonality_detected=seasonality_detected,
            peak_periods=peak_periods[:5],  # Top 5 peaks
            prediction_window=prediction_window,
            predicted_values=predicted_values
        )
    
    def _predict_future_patterns(self) -> Dict[str, Any]:
        """Predict future vulnerability patterns"""
        predictions = {
            "next_month": {},
            "next_quarter": {},
            "risk_assessment": {},
            "recommendations": []
        }
        
        # Use trend analyses for predictions
        if "count_trend" in self.trend_analyses:
            count_trend = self.trend_analyses["count_trend"]
            if count_trend.predicted_values:
                predictions["next_month"]["expected_vulnerabilities"] = int(count_trend.predicted_values[0])
                predictions["next_quarter"]["expected_vulnerabilities"] = int(sum(count_trend.predicted_values[:3]))
        
        if "severity_trend" in self.trend_analyses:
            severity_trend = self.trend_analyses["severity_trend"]
            if severity_trend.predicted_values:
                predictions["next_month"]["expected_avg_severity"] = round(severity_trend.predicted_values[0], 2)
        
        if "exploit_trend" in self.trend_analyses:
            exploit_trend = self.trend_analyses["exploit_trend"]
            if exploit_trend.predicted_values:
                predictions["next_month"]["expected_exploit_rate"] = round(exploit_trend.predicted_values[0], 2)
        
        # Risk assessment based on trends
        risk_factors = []
        if predictions.get("next_month", {}).get("expected_avg_severity", 0) > 7:
            risk_factors.append("High severity vulnerabilities expected")
        if predictions.get("next_month", {}).get("expected_exploit_rate", 0) > 20:
            risk_factors.append("Increased exploit activity predicted")
        
        predictions["risk_assessment"] = {
            "risk_level": "HIGH" if len(risk_factors) >= 2 else "MEDIUM" if risk_factors else "LOW",
            "risk_factors": risk_factors
        }
        
        # Generate recommendations
        if predictions["risk_assessment"]["risk_level"] == "HIGH":
            predictions["recommendations"].extend([
                "Increase security monitoring and patch frequency",
                "Conduct emergency security assessment",
                "Review and update incident response procedures"
            ])
        elif predictions["risk_assessment"]["risk_level"] == "MEDIUM":
            predictions["recommendations"].extend([
                "Maintain regular patching schedule",
                "Monitor security advisories closely",
                "Update security awareness training"
            ])
        else:
            predictions["recommendations"].extend([
                "Continue standard security practices",
                "Schedule quarterly security reviews"
            ])
        
        return predictions
    
    def _identify_emerging_threats(self) -> List[Dict[str, Any]]:
        """Identify emerging threat patterns"""
        emerging_threats = []
        
        # Analyze recent patterns (last 3 months)
        recent_patterns = sorted(
            [(k, v) for k, v in self.temporal_patterns.items() 
             if k.startswith("monthly_")],
            key=lambda x: x[0]
        )[-3:]
        
        if recent_patterns:
            # Find products with increasing vulnerability counts
            product_trends = defaultdict(list)
            for _, pattern in recent_patterns:
                for product in pattern.emerging_products:
                    product_trends[product].append(1)
            
            # Products appearing in multiple recent periods
            for product, appearances in product_trends.items():
                if len(appearances) >= 2:
                    emerging_threats.append({
                        "type": "product_vulnerability_surge",
                        "product": product,
                        "frequency": len(appearances),
                        "severity": "HIGH" if len(appearances) == 3 else "MEDIUM"
                    })
            
            # Check for new threat vectors
            recent_vectors = Counter()
            for _, pattern in recent_patterns:
                recent_vectors[pattern.dominant_vector] += 1
            
            for vector, count in recent_vectors.most_common(2):
                if count >= 2:
                    emerging_threats.append({
                        "type": "threat_vector_increase",
                        "vector": vector.value,
                        "frequency": count,
                        "severity": "MEDIUM"
                    })
        
        return emerging_threats[:5]  # Top 5 emerging threats
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """Calculate comprehensive statistics"""
        if not self.vulnerabilities:
            return {"error": "No vulnerability data available"}
        
        severities = [v.severity for v in self.vulnerabilities]
        
        stats = {
            "total_vulnerabilities": len(self.vulnerabilities),
            "severity": {
                "min": min(severities),
                "max": max(severities),
                "mean": statistics.mean(severities),
                "median": statistics.median(severities),
                "std_dev": statistics.stdev(severities) if len(severities) > 1 else 0
            },
            "by_source": Counter(v.source.value for v in self.vulnerabilities),
            "by_threat_vector": Counter(v.threat_vector.value for v in self.vulnerabilities),
            "exploit_availability": {
                "available": sum(1 for v in self.vulnerabilities if v.exploit_available),
                "percentage": sum(1 for v in self.vulnerabilities if v.exploit_available) / len(self.vulnerabilities) * 100
            },
            "patch_availability": {
                "available": sum(1 for v in self.vulnerabilities if v.patch_available),
                "percentage": sum(1 for v in self.vulnerabilities if v.patch_available) / len(self.vulnerabilities) * 100
            },
            "critical_vulnerabilities": sum(1 for v in self.vulnerabilities if v.severity >= 9.0),
            "high_vulnerabilities": sum(1 for v in self.vulnerabilities if 7.0 <= v.severity < 9.0),
            "medium_vulnerabilities": sum(1 for v in self.vulnerabilities if 4.0 <= v.severity < 7.0),
            "low_vulnerabilities": sum(1 for v in self.vulnerabilities if v.severity < 4.0)
        }
        
        return stats
    
    def _generate_visualizations(self) -> Dict[str, Any]:
        """Generate visualization data for timeline charts"""
        visualizations = {
            "timeline_chart": self._generate_timeline_chart(),
            "severity_heatmap": self._generate_severity_heatmap(),
            "threat_vector_evolution": self._generate_threat_vector_chart(),
            "source_comparison": self._generate_source_comparison()
        }
        
        return visualizations
    
    def _generate_timeline_chart(self) -> Dict[str, Any]:
        """Generate timeline chart data"""
        monthly_data = []
        
        for key, pattern in sorted(self.temporal_patterns.items()):
            if key.startswith("monthly_"):
                monthly_data.append({
                    "period": pattern.period,
                    "count": pattern.count,
                    "avg_severity": round(pattern.avg_severity, 2),
                    "critical_count": pattern.critical_count
                })
        
        return {
            "type": "line_chart",
            "title": "Vulnerability Timeline Analysis",
            "data": monthly_data,
            "x_axis": "period",
            "y_axes": ["count", "avg_severity", "critical_count"]
        }
    
    def _generate_severity_heatmap(self) -> Dict[str, Any]:
        """Generate severity heatmap data"""
        heatmap_data = []
        
        # Group by week and severity level
        for key, pattern in self.temporal_patterns.items():
            if key.startswith("weekly_"):
                heatmap_data.append({
                    "week": pattern.period,
                    "severity_level": self._categorize_severity(pattern.avg_severity),
                    "count": pattern.count
                })
        
        return {
            "type": "heatmap",
            "title": "Weekly Severity Heatmap",
            "data": heatmap_data,
            "x_axis": "week",
            "y_axis": "severity_level",
            "value": "count"
        }
    
    def _categorize_severity(self, severity: float) -> str:
        """Categorize severity score"""
        if severity >= 9.0:
            return "Critical"
        elif severity >= 7.0:
            return "High"
        elif severity >= 4.0:
            return "Medium"
        else:
            return "Low"
    
    def _generate_threat_vector_chart(self) -> Dict[str, Any]:
        """Generate threat vector evolution chart"""
        vector_data = defaultdict(lambda: defaultdict(int))
        
        for key, pattern in sorted(self.temporal_patterns.items()):
            if key.startswith("quarterly_"):
                vector_data[pattern.period][pattern.dominant_vector.value] = pattern.count
        
        # Convert to chart format
        chart_data = []
        for period, vectors in sorted(vector_data.items()):
            data_point = {"period": period}
            data_point.update(vectors)
            chart_data.append(data_point)
        
        return {
            "type": "stacked_bar_chart",
            "title": "Threat Vector Evolution by Quarter",
            "data": chart_data,
            "x_axis": "period",
            "y_axis": "count",
            "categories": [v.value for v in ThreatVector]
        }
    
    def _generate_source_comparison(self) -> Dict[str, Any]:
        """Generate source comparison chart"""
        source_stats = defaultdict(lambda: {
            "count": 0,
            "avg_severity": [],
            "exploit_rate": 0,
            "patch_rate": 0
        })
        
        for vuln in self.vulnerabilities:
            source = vuln.source.value
            source_stats[source]["count"] += 1
            source_stats[source]["avg_severity"].append(vuln.severity)
            if vuln.exploit_available:
                source_stats[source]["exploit_rate"] += 1
            if vuln.patch_available:
                source_stats[source]["patch_rate"] += 1
        
        # Calculate averages
        chart_data = []
        for source, stats in source_stats.items():
            if stats["count"] > 0:
                chart_data.append({
                    "source": source,
                    "count": stats["count"],
                    "avg_severity": round(statistics.mean(stats["avg_severity"]), 2),
                    "exploit_rate": round(stats["exploit_rate"] / stats["count"] * 100, 2),
                    "patch_rate": round(stats["patch_rate"] / stats["count"] * 100, 2)
                })
        
        return {
            "type": "radar_chart",
            "title": "Vulnerability Source Comparison",
            "data": chart_data,
            "metrics": ["count", "avg_severity", "exploit_rate", "patch_rate"]
        }
    
    def _serialize_patterns(self) -> List[Dict[str, Any]]:
        """Serialize temporal patterns for output"""
        serialized = []
        
        for key, pattern in sorted(self.temporal_patterns.items())[:20]:  # Top 20
            serialized.append({
                "period": pattern.period,
                "type": key.split("_")[0],
                "count": pattern.count,
                "avg_severity": round(pattern.avg_severity, 2),
                "critical_count": pattern.critical_count,
                "exploited_count": pattern.exploited_count,
                "patched_percentage": round(pattern.patched_percentage, 2),
                "dominant_vector": pattern.dominant_vector.value,
                "emerging_products": pattern.emerging_products[:3]
            })
        
        return serialized
    
    def _serialize_trends(self) -> Dict[str, Dict[str, Any]]:
        """Serialize trend analyses for output"""
        serialized = {}
        
        for name, trend in self.trend_analyses.items():
            serialized[name] = {
                "trend_type": trend.trend_type,
                "confidence": round(trend.confidence, 3),
                "slope": round(trend.slope, 3),
                "seasonality_detected": trend.seasonality_detected,
                "peak_periods": trend.peak_periods,
                "prediction_window": trend.prediction_window,
                "predicted_values": [round(v, 2) for v in trend.predicted_values]
            }
        
        return serialized
    
    # Wrapper methods for test compatibility
    async def detect_temporal_patterns(self, start_date: datetime, end_date: datetime) -> List[TemporalPattern]:
        """Public wrapper for temporal pattern detection"""
        self._analyze_temporal_patterns(start_date, end_date)
        return list(self.temporal_patterns.values())
    
    async def analyze_trends(self, metric: str, lookback_days: int) -> Dict[str, Any]:
        """Public wrapper for trend analysis"""
        # Generate some vulnerability data for the period
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        # Ensure we have data
        if not self.vulnerabilities:
            await self.analyze_vulnerabilities(
                sources=[VulnerabilitySource.CVE.value],
                start_date=start_date.isoformat(),
                end_date=end_date.isoformat()
            )
        
        self._detect_trends()
        
        # Map metric to trend name
        trend_map = {
            "critical_vulnerabilities": "severity_trend",
            "total_vulnerabilities": "count_trend"
        }
        
        trend_name = trend_map.get(metric, "count_trend")
        if trend_name in self.trend_analyses:
            trend = self.trend_analyses[trend_name]
            return {
                "trend_type": trend.trend_type,
                "confidence": trend.confidence,
                "slope": trend.slope,
                "predicted_values": trend.predicted_values
            }
        
        return {
            "trend_type": "stable",
            "confidence": 0.5,
            "slope": 0.0,
            "predicted_values": [0.0] * 7
        }
    
    async def analyze_threat_vectors(self, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Analyze distribution of threat vectors"""
        # Ensure we have data
        if not self.vulnerabilities:
            await self.analyze_vulnerabilities(
                sources=[VulnerabilitySource.CVE.value],
                start_date=start_date.isoformat(),
                end_date=end_date.isoformat()
            )
        
        vector_counts = Counter(v.threat_vector.value for v in self.vulnerabilities)
        total = len(self.vulnerabilities)
        
        return [
            {
                "vector": vector,
                "count": count,
                "percentage": (count / total * 100) if total > 0 else 0
            }
            for vector, count in vector_counts.most_common()
        ]
    
    async def generate_timeline_visualization(self, start_date: datetime, end_date: datetime, granularity: str) -> Dict[str, Any]:
        """Generate timeline visualization data"""
        # Ensure we have data
        if not self.vulnerabilities:
            await self.analyze_vulnerabilities(
                sources=[VulnerabilitySource.CVE.value],
                start_date=start_date.isoformat(),
                end_date=end_date.isoformat()
            )
        
        # Generate visualization
        viz_data = self._generate_timeline_chart()
        
        # Add required fields
        viz_data["data_points"] = viz_data.get("data", [])
        viz_data["title"] = viz_data.get("title", "Vulnerability Timeline")
        viz_data["x_axis"] = viz_data.get("x_axis", "time")
        viz_data["y_axis"] = viz_data.get("y_axes", ["count"])[0] if viz_data.get("y_axes") else "count"
        
        return viz_data
    
    async def detect_seasonality(self, lookback_months: int) -> Dict[str, Any]:
        """Detect seasonal patterns in vulnerabilities"""
        # Analyze longer period for seasonality
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_months * 30)
        
        # Get data
        await self.analyze_vulnerabilities(
            sources=[VulnerabilitySource.CVE.value, VulnerabilitySource.NVD.value],
            start_date=start_date.isoformat(),
            end_date=end_date.isoformat()
        )
        
        # Check for seasonality in trends
        seasonality_found = False
        peak_periods = []
        
        if "count_trend" in self.trend_analyses:
            trend = self.trend_analyses["count_trend"]
            seasonality_found = trend.seasonality_detected
            peak_periods = trend.peak_periods
        
        return {
            "has_seasonality": seasonality_found,
            "peak_periods": peak_periods,
            "confidence": 0.7 if seasonality_found else 0.3
        }


# Test methods
async def test_basic_timeline_analysis():
    """Test basic vulnerability timeline analysis"""
    print("\nğŸ” Testing Basic Timeline Analysis...")
    
    analyzer = VulnerabilityTimelineAnalyzer()
    
    results = await analyzer.analyze_vulnerabilities(
        sources=['cve', 'nvd'],
        start_date='2024-01-01',
        end_date='2024-06-30',
        parallel_fetch=True
    )
    
    print(f"âœ… Analyzed {results['total_vulnerabilities']} vulnerabilities")
    print(f"ğŸ“Š Found {len(results['temporal_patterns'])} temporal patterns")
    print(f"ğŸ“ˆ Detected {len(results['trend_analyses'])} trends")
    print(f"âš ï¸  Risk Level: {results['predictions']['risk_assessment']['risk_level']}")
    
    return results


async def test_multi_source_analysis():
    """Test analysis with all vulnerability sources"""
    print("\nğŸ” Testing Multi-Source Analysis...")
    
    analyzer = VulnerabilityTimelineAnalyzer()
    
    all_sources = [s.value for s in VulnerabilitySource]
    
    results = await analyzer.analyze_vulnerabilities(
        sources=all_sources,
        start_date='2023-01-01',
        end_date='2024-12-31',
        parallel_fetch=True
    )
    
    print(f"âœ… Analyzed {results['total_vulnerabilities']} vulnerabilities from {len(all_sources)} sources")
    print(f"ğŸ“Š Statistics by source:")
    for source, count in results['statistics']['by_source'].items():
        print(f"   - {source}: {count} vulnerabilities")
    
    return results


async def test_trend_prediction():
    """Test trend detection and prediction capabilities"""
    print("\nğŸ” Testing Trend Prediction...")
    
    analyzer = VulnerabilityTimelineAnalyzer()
    
    # Use longer time period for better trend detection
    results = await analyzer.analyze_vulnerabilities(
        sources=['cve', 'nvd', 'security_advisories'],
        start_date='2022-01-01',
        end_date='2024-12-31',
        parallel_fetch=True
    )
    
    print(f"âœ… Analyzed {results['total_vulnerabilities']} vulnerabilities")
    print(f"\nğŸ“ˆ Trend Analysis:")
    
    for trend_name, trend_data in results['trend_analyses'].items():
        print(f"\n   {trend_name}:")
        print(f"   - Type: {trend_data['trend_type']}")
        print(f"   - Confidence: {trend_data['confidence']:.2%}")
        print(f"   - Slope: {trend_data['slope']:.3f}")
        if trend_data['predicted_values']:
            print(f"   - Next prediction: {trend_data['predicted_values'][0]:.2f}")
    
    print(f"\nğŸ”® Future Predictions:")
    print(f"   - Next month: {results['predictions']['next_month']}")
    print(f"   - Next quarter: {results['predictions']['next_quarter']}")
    
    return results


async def test_emerging_threats():
    """Test emerging threat identification"""
    print("\nğŸ” Testing Emerging Threat Detection...")
    
    analyzer = VulnerabilityTimelineAnalyzer()
    
    results = await analyzer.analyze_vulnerabilities(
        sources=['exploit_db', 'vendor_bulletins'],
        start_date='2024-01-01',
        end_date='2024-12-31',
        parallel_fetch=True
    )
    
    print(f"âœ… Analyzed {results['total_vulnerabilities']} vulnerabilities")
    print(f"\nâš ï¸  Emerging Threats Detected:")
    
    for threat in results['emerging_threats']:
        print(f"\n   - Type: {threat['type']}")
        print(f"   - Severity: {threat['severity']}")
        if 'product' in threat:
            print(f"   - Product: {threat['product']}")
        if 'vector' in threat:
            print(f"   - Vector: {threat['vector']}")
        print(f"   - Frequency: {threat['frequency']}")
    
    return results


async def test_visualization_generation():
    """Test visualization data generation"""
    print("\nğŸ” Testing Visualization Generation...")
    
    analyzer = VulnerabilityTimelineAnalyzer()
    
    results = await analyzer.analyze_vulnerabilities(
        sources=['cve', 'nvd'],
        start_date='2024-01-01',
        end_date='2024-12-31',
        parallel_fetch=False  # Test sequential processing
    )
    
    print(f"âœ… Generated visualizations:")
    
    for viz_name, viz_data in results['visualizations'].items():
        print(f"\n   ğŸ“Š {viz_name}:")
        print(f"   - Type: {viz_data['type']}")
        print(f"   - Title: {viz_data['title']}")
        if 'data' in viz_data:
            print(f"   - Data points: {len(viz_data['data'])}")
    
    return results


async def run_all_tests():
    """Run all test scenarios"""
    print("=" * 80)
    print("ğŸš€ Security Vulnerability Timeline Analysis Tests")
    print("=" * 80)
    
    test_results = []
    
    # Test 1: Basic Timeline Analysis (Expected: ~5 seconds)
    start_time = time.time()
    try:
        result = await test_basic_timeline_analysis()
        test_results.append({
            "test": "Basic Timeline Analysis",
            "status": "âœ… PASS",
            "duration": time.time() - start_time,
            "vulnerabilities": result['total_vulnerabilities']
        })
    except Exception as e:
        test_results.append({
            "test": "Basic Timeline Analysis",
            "status": "âŒ FAIL",
            "duration": time.time() - start_time,
            "error": str(e)
        })
    
    # Test 2: Multi-Source Analysis (Expected: ~8 seconds)
    start_time = time.time()
    try:
        result = await test_multi_source_analysis()
        test_results.append({
            "test": "Multi-Source Analysis",
            "status": "âœ… PASS",
            "duration": time.time() - start_time,
            "vulnerabilities": result['total_vulnerabilities']
        })
    except Exception as e:
        test_results.append({
            "test": "Multi-Source Analysis",
            "status": "âŒ FAIL",
            "duration": time.time() - start_time,
            "error": str(e)
        })
    
    # Test 3: Trend Prediction (Expected: ~12 seconds)
    start_time = time.time()
    try:
        result = await test_trend_prediction()
        test_results.append({
            "test": "Trend Prediction",
            "status": "âœ… PASS",
            "duration": time.time() - start_time,
            "trends": len(result['trend_analyses'])
        })
    except Exception as e:
        test_results.append({
            "test": "Trend Prediction",
            "status": "âŒ FAIL",
            "duration": time.time() - start_time,
            "error": str(e)
        })
    
    # Test 4: Emerging Threats (Expected: ~6 seconds)
    start_time = time.time()
    try:
        result = await test_emerging_threats()
        test_results.append({
            "test": "Emerging Threats",
            "status": "âœ… PASS",
            "duration": time.time() - start_time,
            "threats": len(result['emerging_threats'])
        })
    except Exception as e:
        test_results.append({
            "test": "Emerging Threats",
            "status": "âŒ FAIL",
            "duration": time.time() - start_time,
            "error": str(e)
        })
    
    # Test 5: Visualization Generation (Expected: ~7 seconds)
    start_time = time.time()
    try:
        result = await test_visualization_generation()
        test_results.append({
            "test": "Visualization Generation",
            "status": "âœ… PASS",
            "duration": time.time() - start_time,
            "visualizations": len(result['visualizations'])
        })
    except Exception as e:
        test_results.append({
            "test": "Visualization Generation",
            "status": "âŒ FAIL",
            "duration": time.time() - start_time,
            "error": str(e)
        })
    
    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 80)
    
    total_tests = len(test_results)
    passed_tests = sum(1 for r in test_results if "âœ…" in r["status"])
    
    print(f"\nTotal Tests: {total_tests}")
    print(f"Passed: {passed_tests}")
    print(f"Failed: {total_tests - passed_tests}")
    
    print("\nDetailed Results:")
    for result in test_results:
        print(f"\n{result['test']}:")
        print(f"  Status: {result['status']}")
        print(f"  Duration: {result['duration']:.2f}s")
        if 'error' in result:
            print(f"  Error: {result['error']}")
        else:
            for key, value in result.items():
                if key not in ['test', 'status', 'duration']:
                    print(f"  {key.title()}: {value}")
    
    return test_results


if __name__ == "__main__":
    # Run all tests
    asyncio.run(run_all_tests())