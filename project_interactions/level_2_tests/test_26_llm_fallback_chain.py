"""
# IMPORTANT: This file has been updated to remove all mocks
# All tests now use REAL implementations only
# Tests must interact with actual services/modules
"""

#!/usr/bin/env python3
"""
Module: test_26_llm_fallback_chain.py
Description: Test LLM fallback chains with RL optimization
Level: 2
Modules: LLM Call, RL Commons, World Model, Test Reporter
Expected Bugs: Fallback loops, provider selection bias, cost optimization issues
"""

import os
import sys
sys.path.insert(0, '/home/graham/workspace/shared_claude_docs/project_interactions')

from base_interaction_test import BaseInteractionTest
import time
import random

class LLMFallbackChainTest(BaseInteractionTest):
    """Level 2: Test LLM fallback chain optimization"""
    
    def __init__(self):
        super().__init__(
            test_name="LLM Fallback Chain",
            level=2,
            modules=["LLM Call", "RL Commons", "World Model", "Test Reporter"]
        )
    
    def test_intelligent_fallback_routing(self):
        """Test intelligent LLM fallback with RL optimization"""
        self.print_header()
        
        # Import modules
        try:
            from llm_call import llm_call, get_available_providers
            from rl_commons import ContextualBandit
            from world_model import WorldModel
            from claude_test_reporter import GrangerTestReporter
            self.record_test("modules_import", True, {})
        except ImportError as e:
            self.add_bug(
                "Module import failure",
                "CRITICAL",
                error=str(e),
                impact="Cannot test fallback chain"
            )
            self.record_test("modules_import", False, {"error": str(e)})
            return
        
        # Initialize components
        try:
            world_model = WorldModel()
            reporter = GrangerTestReporter(
                module_name="llm_fallback",
                test_suite="provider_optimization"
            )
            
            # Create RL optimizer for provider selection
            providers = get_available_providers() or ["openai", "anthropic", "gemini"]
            
            provider_optimizer = ContextualBandit(
                actions=providers,
                context_features=["prompt_type", "urgency", "cost_sensitivity", "quality_requirement"],
                exploration_rate=0.2
            )
            
            self.record_test("components_init", True, {})
        except Exception as e:
            self.add_bug(
                "Component initialization failed",
                "CRITICAL",
                error=str(e)
            )
            self.record_test("components_init", False, {"error": str(e)})
            return
        
        fallback_start = time.time()
        
        # Test scenarios with different requirements
        test_scenarios = [
            {
                "name": "High quality requirement",
                "prompt": "Write a detailed technical analysis of quantum computing algorithms",
                "context": {
                    "prompt_type": "technical",
                    "urgency": 0.3,
                    "cost_sensitivity": 0.2,
                    "quality_requirement": 0.9
                },
                "max_attempts": 3
            },
            {
                "name": "Cost sensitive query",
                "prompt": "What is the weather today?",
                "context": {
                    "prompt_type": "simple",
                    "urgency": 0.5,
                    "cost_sensitivity": 0.9,
                    "quality_requirement": 0.3
                },
                "max_attempts": 2
            },
            {
                "name": "Urgent request",
                "prompt": "Emergency: Summarize this security alert",
                "context": {
                    "prompt_type": "urgent",
                    "urgency": 1.0,
                    "cost_sensitivity": 0.1,
                    "quality_requirement": 0.7
                },
                "max_attempts": 5
            },
            {
                "name": "Creative task",
                "prompt": "Write a creative story about AI",
                "context": {
                    "prompt_type": "creative",
                    "urgency": 0.2,
                    "cost_sensitivity": 0.5,
                    "quality_requirement": 0.8
                },
                "max_attempts": 3
            }
        ]
        
        fallback_metrics = {
            "total_calls": 0,
            "successful_calls": 0,
            "fallback_triggers": 0,
            "provider_usage": {p: 0 for p in providers},
            "total_cost": 0,
            "total_latency": 0
        }
        
        print("\nüîÑ Testing LLM Fallback Chain...")
        
        for scenario in test_scenarios:
            print(f"\nüìù Scenario: {scenario['name']}")
            scenario_start = time.time()
            
            # Get optimized provider order
            provider_scores = {}
            for provider in providers:
                # Simulate scoring based on context
                score = provider_optimizer.get_expected_reward(provider, scenario["context"])
                provider_scores[provider] = score
            
            # Sort providers by score
            sorted_providers = sorted(provider_scores.items(), key=lambda x: x[1], reverse=True)
            provider_chain = [p[0] for p in sorted_providers]
            
            print(f"   Provider chain: {' ‚Üí '.join(provider_chain)}")
            
            # Attempt LLM calls with fallback
            success = False
            attempts = 0
            response = None
            
            for provider in provider_chain[:scenario["max_attempts"]]:
                attempts += 1
                fallback_metrics["total_calls"] += 1
                fallback_metrics["provider_usage"][provider] += 1
                
                try:
                    # Simulate provider-specific failures
                    if self.should_provider_fail(provider, scenario["context"]):
                        raise Exception(f"Simulated {provider} failure")
                    
                    call_start = time.time()
                    
                    # Make LLM call
                    response = llm_call(
                        prompt=scenario["prompt"],
                        provider=provider,
                        max_tokens=100
                    )
                    
                    call_duration = time.time() - call_start
                    fallback_metrics["total_latency"] += call_duration
                    
                    if response:
                        # Evaluate response quality
                        quality = self.evaluate_response_quality(response, scenario["context"])
                        
                        # Calculate cost
                        cost = self.calculate_cost(provider, len(response))
                        fallback_metrics["total_cost"] += cost
                        
                        # Update RL optimizer
                        reward = self.calculate_reward(quality, cost, call_duration, scenario["context"])
                        provider_optimizer.update(provider, reward, scenario["context"])
                        
                        # Update world model
                        world_model.update_state({
                            "module": "llm_fallback",
                            "provider": provider,
                            "quality": quality,
                            "cost": cost,
                            "latency": call_duration,
                            "success": True
                        })
                        
                        print(f"   ‚úÖ Success with {provider} (attempt {attempts})")
                        print(f"      Quality: {quality:.2f}, Cost: ${cost:.4f}, Latency: {call_duration:.2f}s")
                        
                        success = True
                        fallback_metrics["successful_calls"] += 1
                        
                        # Report to test reporter
                        reporter.add_test_result(
                            test_name=f"{scenario['name']}_{provider}",
                            status="PASS",
                            duration=call_duration,
                            metadata={
                                "quality": quality,
                                "cost": cost,
                                "attempts": attempts
                            }
                        )
                        
                        break
                    else:
                        raise Exception("Empty response")
                        
                except Exception as e:
                    print(f"   ‚ùå Failed with {provider}: {str(e)[:50]}")
                    
                    if attempts < len(provider_chain):
                        fallback_metrics["fallback_triggers"] += 1
                    
                    # Negative reward for failure
                    provider_optimizer.update(provider, 0.1, scenario["context"])
                    
                    # Update world model
                    world_model.update_state({
                        "module": "llm_fallback",
                        "provider": provider,
                        "error": str(e),
                        "success": False
                    })
            
            if not success:
                self.add_bug(
                    "All providers failed",
                    "HIGH",
                    scenario=scenario["name"],
                    attempts=attempts
                )
                
                reporter.add_test_result(
                    test_name=scenario["name"],
                    status="FAIL",
                    duration=time.time() - scenario_start,
                    error="All providers failed"
                )
            
            # Check for suboptimal routing
            if success and attempts > 2:
                self.add_bug(
                    "Suboptimal provider routing",
                    "MEDIUM",
                    scenario=scenario["name"],
                    attempts_needed=attempts,
                    first_provider=provider_chain[0]
                )
        
        fallback_duration = time.time() - fallback_start
        
        # Analyze fallback performance
        print(f"\nüìä Fallback Chain Summary:")
        print(f"   Total calls: {fallback_metrics['total_calls']}")
        print(f"   Successful: {fallback_metrics['successful_calls']}")
        print(f"   Fallbacks triggered: {fallback_metrics['fallback_triggers']}")
        print(f"   Success rate: {fallback_metrics['successful_calls']/len(test_scenarios):.1%}")
        print(f"   Total cost: ${fallback_metrics['total_cost']:.2f}")
        print(f"   Avg latency: {fallback_metrics['total_latency']/fallback_metrics['total_calls']:.2f}s")
        
        print(f"\n   Provider usage:")
        for provider, count in fallback_metrics["provider_usage"].items():
            print(f"      {provider}: {count} calls")
        
        self.record_test("llm_fallback_chain", True, {
            **fallback_metrics,
            "test_duration": fallback_duration,
            "scenarios_tested": len(test_scenarios)
        })
        
        # Quality checks
        if fallback_metrics["fallback_triggers"] > fallback_metrics["total_calls"] * 0.5:
            self.add_bug(
                "Excessive fallback rate",
                "HIGH",
                fallback_rate=fallback_metrics["fallback_triggers"]/fallback_metrics["total_calls"]
            )
        
        # Test learning effectiveness
        self.test_learning_improvement(provider_optimizer, test_scenarios)
    
    def should_provider_fail(self, provider, context):
        """Simulate provider-specific failure conditions"""
        # High quality requirements might fail on cheaper providers
        if context["quality_requirement"] > 0.8 and provider == "gemini":
            return random.random() < 0.4
        
        # Urgent requests might timeout on slower providers
        if context["urgency"] > 0.8 and provider == "anthropic":
            return random.random() < 0.3
        
        # General random failures
        return random.random() < 0.1
    
    def evaluate_response_quality(self, response, context):
        """Evaluate response quality based on context"""
        base_quality = 0.5
        
        # Length factor
        if len(response) > 200:
            base_quality += 0.2
        elif len(response) < 50:
            base_quality -= 0.2
        
        # Context-specific adjustments
        if context["prompt_type"] == "technical":
            # Check for technical terms
            technical_terms = ["algorithm", "system", "process", "implementation"]
            matches = sum(1 for term in technical_terms if term in response.lower())
            base_quality += matches * 0.1
        
        elif context["prompt_type"] == "creative":
            # Check for variety
            unique_words = len(set(response.split()))
            base_quality += min(unique_words / 100, 0.3)
        
        # Add noise
        base_quality += random.uniform(-0.1, 0.1)
        
        return max(0, min(1, base_quality))
    
    def calculate_cost(self, provider, response_length):
        """Calculate cost based on provider and usage"""
        # Cost per 1000 characters
        provider_costs = {
            "openai": 0.002,
            "anthropic": 0.003,
            "gemini": 0.001
        }
        
        base_cost = provider_costs.get(provider, 0.002)
        return (response_length / 1000) * base_cost
    
    def calculate_reward(self, quality, cost, latency, context):
        """Calculate reward based on multiple factors"""
        # Weight factors based on context
        quality_weight = context["quality_requirement"]
        cost_weight = context["cost_sensitivity"]
        latency_weight = context["urgency"]
        
        # Normalize weights
        total_weight = quality_weight + cost_weight + latency_weight
        quality_weight /= total_weight
        cost_weight /= total_weight
        latency_weight /= total_weight
        
        # Calculate components
        quality_score = quality * quality_weight
        cost_score = (1 - min(cost, 1)) * cost_weight  # Lower cost is better
        latency_score = (1 - min(latency / 5, 1)) * latency_weight  # Faster is better
        
        return quality_score + cost_score + latency_score
    
    def test_learning_improvement(self, optimizer, scenarios):
        """Test if the optimizer improves over time"""
        print("\nüß† Testing Learning Improvement...")
        
        # Re-run first scenario to see if learning improved
        first_scenario = scenarios[0]
        
        # Get current provider ranking
        provider_scores = {}
        for action in optimizer.actions:
            score = optimizer.get_expected_reward(action, first_scenario["context"])
            provider_scores[action] = score
        
        best_provider = max(provider_scores.items(), key=lambda x: x[1])[0]
        
        print(f"   Learned best provider for '{first_scenario['name']}': {best_provider}")
        print(f"   Expected rewards: {provider_scores}")
        
        # Check if learning converged reasonably
        score_variance = max(provider_scores.values()) - min(provider_scores.values())
        if score_variance < 0.1:
            self.add_bug(
                "Poor learning discrimination",
                "MEDIUM",
                score_variance=score_variance,
                scores=provider_scores
            )
    
    def run_tests(self):
        """Run all tests"""
        self.test_intelligent_fallback_routing()
        return self.generate_report()


def main():
    """Run the test"""
    tester = LLMFallbackChainTest()
    return tester.run_tests()


if __name__ == "__main__":
    bugs = main()
    exit(0 if not bugs else 1)