#!/usr/bin/env python3
"""
Module: run_pipeline_test.py
Purpose: Run and report on ArXiv → Marker Pipeline Test

Example Usage:
>>> python run_pipeline_test.py
"""

import sys
import time
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from test_arxiv_marker_pipeline import ArxivMarkerPipelineTest


def generate_test_report(results: List[Dict[str, Any]], total_duration: float) -> str:
    """Generate a comprehensive markdown test report"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report = f"""# ArXiv → Marker Pipeline Test Report
Generated: {datetime.now().isoformat()}

## Test Configuration
- **Test Type**: Level 1 Pipeline Integration
- **Modules**: ArXiv MCP Server → Marker PDF Processor
- **Environment**: Real API calls, Real PDF processing

## Summary

| Test | Description | Result | Status | Duration | Error |
|------|-------------|--------|--------|----------|-------|
"""
    
    # Add each test result
    for result in results:
        status = "✅ Pass" if result["success"] else "❌ Fail"
        error = result.get("error", "") if not result["success"] else ""
        
        report += f"| {result['test_name']} | {result['description']} | {result['summary']} | {status} | {result['duration']:.2f}s | {error} |\n"
    
    # Overall statistics
    passed = sum(1 for r in results if r["success"])
    failed = len(results) - passed
    
    report += f"""
## Overall Results
- **Total Tests**: {len(results)}
- **Passed**: {passed}
- **Failed**: {failed}
- **Success Rate**: {(passed/len(results)*100):.1f}%
- **Total Duration**: {total_duration:.2f}s

## Performance Analysis
"""
    
    # Performance metrics
    for result in results:
        if result["success"] and "performance" in result:
            report += f"""
### {result['test_name']}
- **Papers Processed**: {result['performance'].get('papers_processed', 0)}
- **Average Processing Time**: {result['performance'].get('avg_time_per_paper', 0):.2f}s
- **Quality Score**: {result['performance'].get('quality_score', 0):.2f}
- **Conversion Success Rate**: {result['performance'].get('conversion_rate', 0):.1f}%
"""
    
    # Detailed results
    report += "\n## Detailed Test Results\n"
    
    for i, result in enumerate(results, 1):
        report += f"""
### Test {i}: {result['test_name']}

**Objective**: {result['description']}

**Results**:
"""
        if result["success"]:
            if "details" in result:
                for key, value in result["details"].items():
                    report += f"- **{key}**: {value}\n"
        else:
            report += f"- **Error**: {result.get('error', 'Unknown error')}\n"
            if "traceback" in result:
                report += f"\n```\n{result['traceback']}\n```\n"
    
    # Recommendations
    report += """
## Recommendations

Based on the test results:
"""
    
    if failed > 0:
        report += "1. ❌ **Critical Issues Found** - Pipeline has failures that need addressing\n"
    else:
        report += "1. ✅ **Pipeline Operational** - All tests passed successfully\n"
    
    # Performance recommendations
    avg_time = total_duration / len(results) if results else 0
    if avg_time > 15:
        report += "2. ⚠️ **Performance Optimization Needed** - Average test time exceeds 15 seconds\n"
    else:
        report += "2. ✅ **Good Performance** - Tests complete within acceptable time limits\n"
    
    report += """
## Next Steps

1. Review any failed tests and their error messages
2. Verify PDF quality scores meet requirements (>0.85)
3. Monitor API rate limits for production usage
4. Consider implementing caching for frequently accessed papers

---
*Report generated by ArXiv-Marker Pipeline Test Suite*
"""
    
    return report


def run_comprehensive_tests():
    """Run multiple test scenarios"""
    results = []
    start_time = time.time()
    
    print("=" * 60)
    print("ArXiv → Marker Pipeline Comprehensive Test Suite")
    print("=" * 60)
    
    # Test 1: Basic functionality with common query
    print("\n[Test 1] Basic Pipeline Test")
    print("-" * 40)
    try:
        test1 = ArxivMarkerPipelineTest()
        result1 = test1.run(query="deep learning", max_papers=2)
        
        test1_result = {
            "test_name": "Basic Pipeline Test",
            "description": "Search for deep learning papers and convert to markdown",
            "success": result1.success,
            "duration": result1.duration,
            "summary": f"{len(test1.papers_processed)} papers processed",
            "performance": {
                "papers_processed": len(test1.papers_processed),
                "avg_time_per_paper": result1.duration / max(1, len(test1.papers_processed)),
                "quality_score": test1._calculate_avg_quality(),
                "conversion_rate": (len([r for r in test1.conversion_results if r.get('status') == 'success']) / 
                                  max(1, len(test1.conversion_results))) * 100
            },
            "details": {
                "Papers Found": len(test1.papers_processed),
                "PDFs Downloaded": len([r for r in test1.conversion_results if r]),
                "Successful Conversions": len([r for r in test1.conversion_results if r.get('status') == 'success']),
                "Average Quality": f"{test1._calculate_avg_quality():.2f}"
            }
        }
        results.append(test1_result)
        test1.teardown()
    except Exception as e:
        import traceback
        results.append({
            "test_name": "Basic Pipeline Test",
            "description": "Search for deep learning papers and convert to markdown",
            "success": False,
            "duration": time.time() - start_time,
            "summary": "Test failed",
            "error": str(e),
            "traceback": traceback.format_exc()
        })
    
    # Test 2: Complex query with specific topic
    print("\n[Test 2] Complex Query Test")
    print("-" * 40)
    try:
        test2 = ArxivMarkerPipelineTest()
        result2 = test2.run(query="quantum computing error correction", max_papers=2)
        
        test2_result = {
            "test_name": "Complex Query Test",
            "description": "Search for specific technical papers on quantum computing",
            "success": result2.success,
            "duration": result2.duration,
            "summary": f"{len(test2.papers_processed)} papers processed",
            "performance": {
                "papers_processed": len(test2.papers_processed),
                "avg_time_per_paper": result2.duration / max(1, len(test2.papers_processed)),
                "quality_score": test2._calculate_avg_quality(),
                "conversion_rate": (len([r for r in test2.conversion_results if r.get('status') == 'success']) / 
                                  max(1, len(test2.conversion_results))) * 100
            },
            "details": {
                "Papers Found": len(test2.papers_processed),
                "Query Complexity": "High (3+ terms)",
                "Technical Domain": "Quantum Computing",
                "Conversion Quality": f"{test2._calculate_avg_quality():.2f}"
            }
        }
        results.append(test2_result)
        test2.teardown()
    except Exception as e:
        import traceback
        results.append({
            "test_name": "Complex Query Test",
            "description": "Search for specific technical papers on quantum computing",
            "success": False,
            "duration": time.time() - start_time,
            "summary": "Test failed",
            "error": str(e),
            "traceback": traceback.format_exc()
        })
    
    # Test 3: Recent papers test
    print("\n[Test 3] Recent Papers Test")
    print("-" * 40)
    try:
        test3 = ArxivMarkerPipelineTest()
        # Search for very recent AI papers
        result3 = test3.run(query="artificial intelligence 2024", max_papers=1)
        
        test3_result = {
            "test_name": "Recent Papers Test",
            "description": "Verify handling of recently published papers",
            "success": result3.success,
            "duration": result3.duration,
            "summary": f"{len(test3.papers_processed)} recent papers processed",
            "performance": {
                "papers_processed": len(test3.papers_processed),
                "avg_time_per_paper": result3.duration / max(1, len(test3.papers_processed)),
                "quality_score": test3._calculate_avg_quality(),
                "conversion_rate": 100.0 if test3.conversion_results else 0.0
            },
            "details": {
                "Papers Found": len(test3.papers_processed),
                "Time Filter": "Recent (2024)",
                "Processing Speed": f"{result3.duration:.2f}s",
                "Output Quality": "High" if test3._calculate_avg_quality() > 0.85 else "Medium"
            }
        }
        results.append(test3_result)
        test3.teardown()
    except Exception as e:
        import traceback
        results.append({
            "test_name": "Recent Papers Test",
            "description": "Verify handling of recently published papers",
            "success": False,
            "duration": time.time() - start_time,
            "summary": "Test failed",
            "error": str(e),
            "traceback": traceback.format_exc()
        })
    
    total_duration = time.time() - start_time
    
    # Generate and save report
    report = generate_test_report(results, total_duration)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_dir = Path(__file__).parent.parent.parent.parent / "docs" / "reports"
    report_dir.mkdir(parents=True, exist_ok=True)
    
    report_path = report_dir / f"arxiv_marker_pipeline_test_report_{timestamp}.md"
    report_path.write_text(report)
    
    # Also save JSON results for programmatic access
    json_path = report_dir / f"arxiv_marker_pipeline_test_results_{timestamp}.json"
    json_results = {
        "timestamp": datetime.now().isoformat(),
        "total_duration": total_duration,
        "results": results,
        "summary": {
            "total_tests": len(results),
            "passed": sum(1 for r in results if r["success"]),
            "failed": sum(1 for r in results if not r["success"]),
            "success_rate": (sum(1 for r in results if r["success"]) / len(results) * 100) if results else 0
        }
    }
    json_path.write_text(json.dumps(json_results, indent=2))
    
    # Print summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    print(f"Total Tests: {len(results)}")
    print(f"Passed: {sum(1 for r in results if r['success'])}")
    print(f"Failed: {sum(1 for r in results if not r['success'])}")
    print(f"Total Duration: {total_duration:.2f}s")
    print(f"\nReports saved to:")
    print(f"  - Markdown: {report_path}")
    print(f"  - JSON: {json_path}")
    
    # Return exit code
    return 0 if all(r["success"] for r in results) else 1


if __name__ == "__main__":
    exit(run_comprehensive_tests())